{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Evaluating machine-learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the three examples presented in chapter 3, we split the data into a **training set**, a **validation set**, and a **test set**. The reason not to evaludate the models on the same data  they were trained on quickly became evident: after just a few epochs, all three models began to **overfit**. That is, their performance on never-before-seen data started stalling (or worsening) compared to their performance on the training data - which always improves as training progresses. <br>\n",
    "In machine learning, the goal is to achieve models that **generalize** - that perform well on never-before-seen data - and overfitting is the central obstacle. You can only control that which you can observe, so it's crucial to be able to reliably measure the generalization power of your model. The following sections look at strategies for mitigating overfitting and maximizing generalization.<span class=\"mark\">In this section, we'll focus on how to measure generalization : how to evaluate machine-learning models.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 Training, validation, and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating a model always boils down to splitting the available data into three sets:\n",
    "- training data\n",
    "- validation data.\n",
    "- test data.\n",
    "\n",
    "You train on the training data and evaludate your model on the validation data. Once your model is ready for prime time, you test it one final time on the test data. <br>\n",
    "You may ask, why not have two sets: a training set and a test set? You'd train on the training data and evaludate on the test data. Much simpler!\n",
    "The reason is that developing a model always involves **tuning its configuration**: for example, choosing the number of layers or the size of the layers (called the **hyper-parameters** of the model, to distinguish them from the **parameters**, which are the network's weights). You do this tuning by using as a feedback signal the performance of the model on the validation data. In essence, this tuning is a form of **learning** : search for a good configuration in some parameter space. As a result, tuning the configuration of the model based on its performance on the validation set can quickly result in **overfitting to the validation set**, even though your model is never directly trained on it.\n",
    "Central to this phenomenon is the notion of **information leaks**. Every time you tune a hyperparameter of your model based on the model's performance on the validation set, some information about the validation data leaks into the model. some information about the validation data leaks into the model. If you do this only once, for one parameter, then very few bits of information will leak, and your validation set will remain reliable to evaludata the model. But if you repeat this many times - running one experiment, evaluating on the validation set, and modifying your model as a result - then you'll leak an increasingly significant amount of information about the validation set into the model.<br>\n",
    "At the end of the day, you'll end up with a model that performs artificially well on the validation data, because that's what you optimized it for. **You care about performance on completely new data**. not the validation data, <span class=\"mark\">so you need to use a completely different, never-before-seen dataset to evaluate the model: the test data set.</span>your model shouldn't have had access to any information about the test set, even indirectly.\n",
    "If anything about the model has benn tuned based on test set performance, then your measure of generalization will be flawed. <br>\n",
    "**Splitting your data into training, validation, and test sets** may seem straightforward, but there are a few advanced ways to do it that can come in handy when little data is available. Let's review three classic evaluation recipes: simple hold-out validation, K-fold validation, and iterated K-fold validation with shuffling.\n",
    "\n",
    "### Simple HOLD-OUT VALIDATION\n",
    "Set apart some fraction of your data as your test set. Train on the remaining data, and evaludate on the test set. As you saw in the previous sections, in order to prevent information leaks, you shouldn't tune your model based on the test set, and therefore you should also reserve a validation set.<br>\n",
    "Schematically, hold-out validation look figure 4.1. The following listing shows a simple implmentation.\n",
    "![4.1](nb_images/Figure 4.1.JPEG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T14:05:30.115268Z",
     "start_time": "2018-06-17T14:05:30.111278Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing 4.1 Hold-out validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = 10000\n",
    "# Shuffling the data is usually appropriate.\n",
    "np.random.shuffle(data) \n",
    "# Defines the validation set\n",
    "validation_data = data[:num_validation_sample]\n",
    "data = data[num_validation_samples]\n",
    "\n",
    "# Defines the training set\n",
    "training_data = data[:]\n",
    "\n",
    "# Trains a model on the training data, and evaludates it in the validation data\n",
    "model = get_model()\n",
    "model.train(training_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "\n",
    "# At this point you can tune your model,\n",
    "# retrain it, evaludate it, tune it again...\n",
    "\n",
    "# Once you've tuned your hyperparameters, \n",
    "# it's common to train your final model from scratch on all non-test data available.\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data, validation_data]))\n",
    "test_score = model.evaludata(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shuffling the data is usually appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T14:10:46.008101Z",
     "start_time": "2018-06-17T14:10:45.994142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78271 63528 57890 ... 37304 46322 75090]\n"
     ]
    }
   ],
   "source": [
    "data = np.arange(0, 80000)\n",
    "num_validation_samples = 10000\n",
    "np.random.shuffle(data)\n",
    "print(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simplest evaludation protocol, and it suffers from one flaw: **if little data is available, then your validation and test sets may contain too few samples to be statistically representative of the data at hand**. This is easy to recognize: if different random shuffling rounds of the data before splitting end up yielding very different measures of model performance, then  you're having this issue. K-fold validation and iterated K-fold validation are two ways to address this, as discussed next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-FOLD VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
