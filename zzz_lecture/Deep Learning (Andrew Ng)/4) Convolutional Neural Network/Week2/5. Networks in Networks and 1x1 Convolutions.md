## 5. Networks in Networks and 1x1 Convolutions ##
http://gaussian37.me/221152478731

[동영상 링크](http://serviceapi.nmv.naver.com/flash/convertIframeTag.nhn?vid=35F63D53A57F8A40EAC94AA1B1F1445AA003&outKey=V12290c76988a98ae1b6e52aca63efe78676cc00891a64ef60d9552aca63efe78676c&width=936&height=526)

Network  architecture를 설계할 때, 1 x 1  convolution을 사용하는 것은 상당히 도움이 됩니다. 

1 x 1 convolution은 무었일까요?

![1](https://i.imgur.com/TxJePTx.png)


1 x 1(정확히 1 x 1 x 1 필터) 필터에 숫자 2를 할당하였습니다. 그리고 6 x 6 이미지(정확히 6 x 6 x 1 이미지)를 1 x 1 필터와 convolution 연산을 하도록 하겠습니다.

![2](https://i.imgur.com/xYg7IcE.png)

이 때 연산의 결과는 마치 스칼라 곱과 같습니다.

![3](https://i.imgur.com/kvRnGpt.png)

반면 위와 깉이 6 x 6 x 1이 아닌 6 x 6 x <span style="color:red">**32**</span>인 경우에는 상황이 달라집니다. 

1 x 1 x 1 필터에서는 마치 스칼라 곱과 같았지만, 1 x 1 x <span style="color:red">**32**</span> 필터를 사용할 때에는 32개의 다른 위치의 값들과

연산을 하게 됩니다. 그리고 Non-linearity 성질을 추가하기 위해 Relu 함수를 적용하도록 합시다.

![4](https://i.imgur.com/yORXXVB.png)

1 x 1 x 32 필터를 특정 위치에서 element-wise 연산을 한 후 Relu를 적용하면 하나의 스칼라 값을 구할 수 있습니다.

![5](https://i.imgur.com/m4Zk5mb.png)

만약에 1 x 1 x 32 필터가 #filter 만큼 있다면 output의 결과는 당연히 6 x 6 x #filter가 됩니다.

위 아이디어가 기본적인 1 x 1 convolution에 해당합니다. (또는 Network in Network 라고도 합니다.)

이 아이디어는 현재 다른 Neural Network 아키텍쳐들에도 널리 사용 중입니다.

특히 유명한 inception network에서도 사용중입니다.

![6](https://i.imgur.com/Qwn6WXV.png)

1 x 1 convolution layer가 유용하게 사용되는 예시를 살펴보도록 하겠습니다.

먼저 input layer의 shape이 28 x 28 x 192 라고 합시다. 일반적으로 height, width를 줄이려면 pooling layer를 이용하면 됩니다. (stride 도 가능합니다.)

만약 channel의 수가 너무 많아서 위와 같이 28 x 28 x 192 → 28 x 28 x 32 로 축소하고 싶으면 어떻게 하면 될까요?

이 때 1 x 1 convolution을 사용하면 됩니다.

![7](https://i.imgur.com/7njNlHN.png)

28 x 28 x 192 shape을 가지는 input layer를 28 x 28 x 32로 변경하려면 1 x 1 x 192 필터를 32개(#filter) 가지고 있어야 합니다. 즉 (1, 1, 192, 32) shape의 필터가 필요합니다. 

이 때, input layer와 filter의 convolution 연산 결과 output layer는 28(n<sub>h</sub>) x 28(n<sub>w</sub>) x 32(n<sub>c</sub>)가 됩니다.

결과적으로 height, width 는 유지가 되고 volume은 축소가 됩니다. 또한 1 x 1 convolution으로 channel 수를 줄일 수 있기 때문에 네트워크 내에서의 연상량 또한 줄일 수 있습니다.

![](https://i.imgur.com/HmQfvPx.png)

만약 volume을 그대로 유지하고 싶으면 #filter = 192를 사용하여 channel을 유지하면 됩니다. 

volume은 유지하더라도 1 x 1 convolution에서 사용하는 Relu로 인하여 non-linearity 성질을 추가할 수 있습니다.


# 마지막으로 정리하면 1 x 1 convolution은 non-linearity 성질을 더하면서 output volume을 증가/유지/감소 할 수 있습니다. #

다음 강의 에서는 inception network에서 1 x 1 convolution이 어떻게 사용되는지 알아보도록 하겠습니다.