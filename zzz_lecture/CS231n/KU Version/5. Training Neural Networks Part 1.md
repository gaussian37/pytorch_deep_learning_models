## 5. Training Neural Networks Part 1 ##

※ 출처 : http://dsba.korea.ac.kr/wp/?page_id=167

※ 영상 : [링크](https://www.youtube.com/watch?v=a8jEXBAhRXI&list=PLetSlH8YjIfXMONyPC1t3uuDlc1Mc5F1A&index=5)

※ 작성자 : 김진솔

![1](https://i.imgur.com/xMR853Z.png)

지난 시간에는 add, max, multiply gate에서의 back-propagation과 sigmoid 자체 back-propagation의 결과를 구하여 add, max, multiply의 조합으로 sigmoid 를 만들었을 때의 back-propagation 결과와 sigmoid 자체 back-propagation 결과가 같음을 확인하였습니다.

※ 위의 sigmoid 미분 식에서 음의 부호가 중간에 생략됨. 결과는 맞음

![2](https://i.imgur.com/xCeE0Zy.png)

네트워크를 학습하다 보면 이전 layer의 한 줄기가 아닌 여러 줄기에서 뻗어나온 back-propagation들의 gradient가 합쳐지게 됩니다.

이러한 경우에 gradient들이 전부 더해져서 들어오게 됨을 학습하였습니다. 

더해서 들어오는 것을 학습하게 되면 ResNet 같은 구조에서 Skip connection에서 gradient가 결국 +(add)로 올라가서 합쳐짐을 알 수 있었습니다. 

bottleneck 구조에서 또한 dimension이 달라지더라도 1 x 1 convolution & stride 2로 확장하는 구조이기 때문에 back-propagation 시 gradient가 상위 layer로 올라가서 add 되는 것을 알 수 있었습니다.

![3](https://i.imgur.com/6DxAlvf.png)

요즘 모델의 추세는 FC를 많이 만들기 보다는 <span style="background-color: #FFFF00">CNN 에서 FC 보다 3x3 conv를 많이 사용하고 마지막에 GAP(Global Average Pooling)을 하는 경향</span>이 있습니다.

Weakly Supervised Learning을 할 때 Max pooling 보다는 GAP를 하면 더 좋은 결론이 나온다는 것이 알려져 있습니다.

![4](https://i.imgur.com/E55ewed.png)

back-propagation에 대한 수식을 상세히 보면 위와 같습니다. 위 수식에서 사용된 g(x)는 sigmoid가 사용되었고 cost function은 MSE가 사용되었다고 가정하고 수식을 전개하면 위와 같습니다. 

※ g(x) 는 sigmoid 함수로 미분 방법은 위에서 참조 바랍니다.

![5](https://i.imgur.com/ypkyMUn.png)

이전 슬라이드에서는 W2, b2에 대하여 미분을 하였다면 이번에는 W1, b1으로 미분한 결과 입니다. 이전 슬라이드 결과와 유사하게 결국에는 δ 형태로 묶을 수 있습니다. 

![6](https://i.imgur.com/PgOVSMA.png)

즉 위와 같이 chain rule과 같아지게 됩니다. 따라서 Add/Max/Multiply gate를 이용하여 구성한 Network에서의 back-propagation 과 결국 같은 값을 가지게 됩니다. 

![7](https://i.imgur.com/NJexDol.png)

Network 구조에 대하여 잠시 살펴 보면 FC(Fully Connected)가 있을 것이고 LC(Locally Connected)가 있을 것입니다. Convolution 연산은 FC/LC 측면에서 바라보면 LC에 해당하고 특히 weight를 공유하므로 shared-weight local 구조라고 할 수 있습니다.

![8](https://i.imgur.com/fAtnLCw.png)

CNN은 결국 Shared weight를 사용하고 Shared weight들에 대하여 gradient가 다 합쳐지는 것이라고 보면 됩니다.

![9](https://i.imgur.com/Ldf9m3L.png)

사용하고 싶은 데이터 (your data)가 있고, ImageNet 데이터를 이용하여 Pre-Train 된 데이터가 있을 때, 이 데이터를 초기 조건으로 사용하고 사용자가 사용할 데이터를 fine tuning 하여 Transfer Learning으로 사용할 수 있습니다.

 ImageNet 같은 유명한 모델들은 코드들을 구할 수 있습니다. 


> Transfer learning을 할 때 나의 데이터의 양이 많지 않다면, 끝단을 수정하고, 데이터의 양이 어느 정도 있다면 중간 부분을 수정하여 training을 하면 더 좋다고 알려져 있습니다. (무슨말인지...)

Transfer Learning 시 고려할 점은 다음과 같습니다.

1) Pre-train data와 추가할 data의 이질성이 심한 경우 사용을 자제 해야 한다.

2) Fine Tuning을 할 때 사용하는 데이터의 양에 따라 사용방법을 다르게 해야한다. (데이터의 양은 상대적임)

![10](https://i.imgur.com/IS05nY2.png)

CNN 에서는 Convolution연산과 Activation 연산이 하나의 집합이 되고 그 다음 Pooling이 일어나게 됩니다. 이렇게 만들어진 집합을 반복하게 됩니다. 그리고 마지막으로 FC를 적용하게 됩니다. 그리고 앞에서 언급한 바와 같이 최근 추세는 다음과 같습니다.

① 3 x 3 필터 여러번 사용 (큰 사이즈 필터 사용 안함)

② FC를 적게 사용하며 마지막 Conv 에서는 GAP을 사용

![11](https://i.imgur.com/0Jt82tt.png)

Activation Function에 대하여 알아보겠습니다. 한 layer의 결과가 activation function으로 들어가게 됩니다. 이 때 Σw*x + b의 형태가 선형 조합이기 때문에 비선형 조합으로 만들어 주어야 합니다.  

![12](https://i.imgur.com/UYDlgxo.png)

만약 Activation function을 Linear function을 사용한다면 layer을 계속 쌓더라도 선형 형태가 되어 의미가 없어 집니다.

![13](https://i.imgur.com/6j9wwdl.png)

그래서 통상적으로 Activation function은 Non-Linear function을 사용하게 됩니다. 

![14](https://i.imgur.com/ZNWZpF4.png)

Activation function은 TF 에서 다양하게 제공하고 있습니다.

![15](https://i.imgur.com/odeGcj5.png)

먼저 Sigmoid function 부터 살펴보겠습니다.

① 입력 값이 일정 범위의 safety zone을 넘어가게 되면 0 또는 1로 수렴하게 되고 gradient(경사값) 또한 0으로 수렴해 버리게 됩니다. 양 끝이 평평해 지기 때문입니다.

② sigmoid function의 범위가 [0, 1]입니다. 이 때문에 output의 중앙값이 0이 아니게 됩니다. 0을 기준으로 데이터가 분포하게 되었을 때가 이상적인데 Sigmoid에서는 하나의 단점이 됩니다.

③ Relu와 비교해 보았을 때, exp() 연산에 많은 cost가 듭니다.

![16](https://i.imgur.com/OVwpK6L.png) 

파란색 부분이 safety zone 입니다. safety zone을 넘어서게 되면 gradient 자체가 0으로 수렴하게 됩니다. 

![17](https://i.imgur.com/VjHPEgO.png)

Sigmoid가 Non-zero centered 분포를 가지게 되는 문제는 만약 input neuron이 항상 positive 라면 w에 대한 gradient가 항상 positive 또는 negative가 되게 학습이 잘 안될 수 있습니다. 학습하기 좋으려면 cost function이 minimize 되는 지점을 다양한 gradient 크기로 찾아가야 하는데 항상 positive/negative 하면 minimize 지점을 찾기가 힘들어 집니다.

![18](https://i.imgur.com/9l2aLxS.png)

exponential 자체가 상당히 계산하기 비싼 term 이기 때문에 forward/backward 시 계산이 오래 걸립니다. 

![19](https://i.imgur.com/8KdWRaS.png)

tanh(x)은 sigmoid function의 2번째 문제인 non-zero centered 문제만 해결하고 나머지 문제는 해결을 못하였습니다. 

![20](https://i.imgur.com/16yLbhY.png)

Relu는 sigmoid와 달리 exponential term이 없고 max function 하나만 사용하므로 sigmoid/tanh 보다 계산 시 6배 정도 빠르다고 알려져 있습니다. 
Relu에서는 gradient는 어떻게 될까요?

x = -10일 때는 gradient = 0 입니다.

x = 10 일 때에는 gradient = 1 입니다. 

x = 0 일 때에는 좌/우 극한이 다르므로 의미가 없습니다.

![21](https://i.imgur.com/LssEYqI.png)

- Leaky Relue : Relu에서는 Negative term에 대해서 모두 gradient가 0이 되지만 Leaky Relu 에서0이 되지 않도록 합니다. 

- PRelu :  Leaky Relu에서 사용된 negative 입력에 대한 gradient α 또한 학습을 하여 정하는 모델입니다.

![22](https://i.imgur.com/PF0NtEL.png)

ELU는 Relu에서 nagative 입력에 한하여 exponential term을 추가하는 형태 입니다. Relu의 장점과 더불어 gradient가 0으로 죽지 않고 output의 평균값이 zero에 가까워 지는 장점을 가지게 되지만 exponential term의 추가로 계산량이 증가하게 됩니다.

![23](https://i.imgur.com/OU9aTsD.png)

만약에 Weight를 전부 0으로 초기화 한다면 어떻게 될까요? 결과는 좋지 않습니다. 왜냐하면 back propagation 시 연산되는 결과 같이 모두 같아져 버리게 됩니다. 

![24](https://i.imgur.com/hETDcbB.png)

위는 weight initialize에 대한 cs231n의 실험한 코드 입니다.

![25](https://i.imgur.com/dnrhxmQ.png)

첫번째 weight initialization 테스트는 표준 정규 분포에 0.01을 곱한 초기화 입니다. 

이 때 결과를 보면 layer가 커지면 커질수록 std가 0에 수렴하여 weight들이 0에 가깝게 분포하는것을 볼 수 있습니다.

이 때 발생할 수 있는 문제점은 2가지 입니다.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1) node들의 갯수가 많음에도 불구하고 weight가 0에 수렴해 버리기 때문에 다양한 node의 의미가 없어지게 됩니다.
 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;즉 다양성이 줄어듭니다.

&nbsp;&nbsp;&nbsp;&nbsp;2) back propagation 시 gradient가 weight값을 곱해져서 전파되기 때문에 weight가 0에 가까워 져버리면 전달되는 값이 없어지게 됩니다. 

![26](https://i.imgur.com/hvXZDIx.png)

이번에는 표준 정규 분포에 1.0을 곱하여 이전 슬라이드의 0.01보다 상대적으로 큰 값으로 초기화를 주었습니다. 

분포 값에 작은 값을 곱하면 layer가 깊어질수록 모든 weight 값이 0에 수렴하였었지만 이번에는 큰 값인 양극단에 분포하게 됨을 알 수 있습니다.

gradient가 매우 커져 버린 weight와 곱해지게 되므로 activation에서 Vanishing gradient 문제를 가지게 됩니다.

![27](https://i.imgur.com/09e1nO8.png)

**Xavier Initialization :** 하나의 노드 관점에서 보았을 때, input/output weight의 갯수가 다른데, 똑같은 표준 정규 분포로 초기화 하는 것이 합당한가에 대한 질문에서 Xavier Initialization은 시작 됩니다. 

Xavier Initialization은 **activation이 linear 하다고 가정**할 때, std deviation이 1/(sqrt(n)) 을 주는 것이 가장 합당하다는 것을 수학적으로 증명한 방법입니다. 

결과를 보면 layer 마다 앞서 나온 문제가 안 생기고 넓게 분포가 퍼져 있음을 알 수 있습니다.

![28](https://i.imgur.com/gWk7aSq.png)

**He Initialization** : He Initialization은 Xavier Initialization과 비교 시 변한 term은 사용된 std deviaton입니다. 1/(sqrt(n)) → 1/(sqrt(n/2)) 로 변경되었습니다.

Xavier Initialization의 경우 Linear activation을 가정하였기 때문에 과연 그것이 옳은가에 대한 의문을 가지게 되었고(일반적으로 Linear activation은 안쓰고 Relu를 쓰기 때문) 표준 편차를 더 크게 가져서 더 넓은 분포를 가지게 하였습니다.

![29](https://i.imgur.com/qXeGDWQ.png)

<span style="background-color: #FFFF00">Xavier Initialization 보다 He Initialization이 큰 데이터에서는 적은 epoch으로 빨리 수렴이 되는 성과를 보였고  Xavier Initialization은 작은 모델로 학습하였을 때 학습이 안되는 것을 볼 수 있었습니다.</span>

![30](https://i.imgur.com/ORNqhBU.png)

<span style="background-color: #FFFF00">CNN에서 filter 파라미터에는 어떻게 He Initialization을 사용할까요?</span>

먼저 convolution filter는 (width, height, in, out) 4-dimension의 tensor를 가집니다.

따라서 std deviation을 1/sqrt( (w*h*#in) / 2) 로 계산하여 filter에 He Initialization을 적용할 수 있습니다.

![31](https://i.imgur.com/ovDBk0B.png)

Batch Normalization은 최근 CNN에서 default로 사용되는 중요한 방법입니다.

![32](https://i.imgur.com/ERN8BE4.png)

Batch Normalization에 나오는 내용으로 Internal Covariate Shift가 나오게 됩니다.<br/>
계속 Batch 단위로 training을 하게 되면, Batch에 따라 계속 분포가 변하게 되고, 현재 training의 결과와 이전 traingin의 결과가 layer가 깊어질수록 점점 더 변화량이 누적되어 너무 커지게 되는 문제가 발생합니다. 

![33](https://i.imgur.com/a20W5oP.png)

Batich Normalization을 구하는 방법은 위와 같습니다.</br>
각 Batch에 대하여 mean, variance를 구하여 normalization을 합니다. 이 때 ε 값을 더하여 분모가 0이 되는것을 방지 합니다.</br>
이렇게 구한 normalize 값에 γ(scale parameter)와 β(shift parameter)를 각각 곱하고 더하게 되어 output y를 구합니다.
이 때 γ와 β 는 학습을 통하여 정하게 됩니다.</br>
논문의 결과를 보면 Batch norm을 적용하였을 때 converge가 더 빨리 됨을 볼 수 있습니다.</br>높은 learning rate 사용 가능, initialization에 덜 의존적으로 되는 장점도 있습니다. 또한 regularization effect로 drop out의 필요성을 줄이게 됩니다.

![34](https://i.imgur.com/3D8HADt.png)
Batch Normalization에 대하여 상세히 알아보겠습니다.</br>
처음에는 γ = 1, β = 0 의 값으로 시작합니다. γ = 1, β = 0 값은 어떠한 값 변화도 발생하지 않습니다.</br> 
위 자료는 gate에 따른 Batch Normalization의 Forward/Backward Propagation 입니다.

![35](https://i.imgur.com/9WxWp24.png)
위에는 bath normalization에 관한 간단한 numpy 코드 입니다.

![36](https://i.imgur.com/hEAtQbs.png)
Back propagation에 대한 코드는 위와 같습니다.</br> 
※ 참고로 자료 gate 에서 sqrt(x - ε) => sqrt(x + ε) 로 고쳐야 합니다.

![37](https://i.imgur.com/lhcmWkL.png)
Forward Propagation으로 계산을 진행할 때, 결과값과 중간값(cache)을 저장합니다.</br>
최종적으로 loss를 구한 다음에 back-propagation으로 local gradient를 계산할 때 중간값(cache)을 이용합니다.

![38](https://i.imgur.com/2X88VNq.png)
Local gradient를 구하는 방법에 대하여 좀 더 자세하게 살펴보겠습니다.</br>
Add gate이므로 local gradient를 그대로 분배하는 역할을 하나 β의 경우 row 방향으로 합하여 벡터로 만듭니다. 각 변수에 대하여 β gradient가 계산되게 됩니다. 

![39](https://i.imgur.com/dp83AKV.png)

multiply gate이므로 gradient에 각각의 input을 교차해서 곱해줍니다.</br>
β 와 마찬가지로 γ 에서도 np.sum을 이용하여 row 기준 합으로 처리해줍니다.

![40](https://i.imgur.com/AsuoUB7.png)

Step8과 마찬가지로 Multiply gate의 back-propagation 시 gradient가 들어오면 input이 switch된 후 곱해지게 됩니다.</br>
아래쪽의 divar은 std deviation을 구하는 것으로 batch normalization 단위가 batch 단위로 구하는 것이기 때문에 batch 단위로 Σ로 summation을 해줍니다. 

![41](https://i.imgur.com/dnhjkbJ.png)

역수 연산을 한 gate에서는 역수에 대한 미분값을 계산하여 전파를 합니다.

![42](https://i.imgur.com/KGEIaAG.png)

sqrt(x + ε) 에 대한 미분값을 취하여 gradient를 전파합니다.

![43](https://i.imgur.com/4jU5zHi.png)

Σ 로 합쳐지기 전의 상태로 재 분배를 해주는 과정을 통하여 gradient를 전파합니다.</br>
합쳐지기 전의 상태로 원복해야 하기 때문에 np.one을 이용하여 모든 원소가 1인 (N,D) shape의 행렬을 만든 후 gradient를 곱하여 앞 layer로 전파합니다.

![44](https://i.imgur.com/qr4Y2jF.png)

square 연산의 경우 간단하게 미분한 결과인 x2 를 gradient에 연산한 다음 전파합니다.

![45](https://i.imgur.com/XrgV3GN.png)

"-" gate에서의 forward 결과가 x - μ 였습니다. - gate의 back-propagation은 전파하기 전에 뒤의 term(여기서는 μ)에 -1을 곱한 후 전파하면 됩니다.

![46](https://i.imgur.com/c49lmpf.png)

step1에서는 step4와 마찬가지로 Σ 하기전의 상태로 복원을 해주면 됩니다.

![47](https://i.imgur.com/DoO6IRb.png)

마지막으로 시작점에서는 이전 까지의 결과를 합쳐주면 됩니다.</br>
ResNet에서도 Input → BN → Activation Function을 써주는 것을 볼 수 있습니다.

![48](https://i.imgur.com/nGJRJwA.png)

TF를 이용하여 위와 같이 코드를 작성할 수있습니다. 이 때 BN을 적용한 값과 moving average를 리턴하여 추후에 같이 사용해야 합니다.</br>
이왕 TF를 사용하려면 좀 더 high-level로 사용하여도 될 것 같습니다.

    tf.layers.batch_normalization(inputs,
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer=tf.zeros_initializer(),
    gamma_initializer=tf.ones_initializer(),
    moving_mean_initializer=tf.zeros_initializer(),
    moving_variance_initializer=tf.ones_initializer(),
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    training=False,
    trainable=True,
    name=None,
    reuse=None,
    renorm=False,
    renorm_clipping=None,
    renorm_momentum=0.99,
    fused=None
    )
    
      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
      with tf.control_dependencies(update_ops):
    train_op = optimizer.minimize(loss)

![49](https://i.imgur.com/MUXkSwC.png)
처음에 training 할 때 hyper parameter를 어떻게 잡을지에 대한 내용입니다.</br>
데이터 자체에 전처리를 하는 방법입니다. 데이터를 normalization 해주는 전처리 입니다.

![50](https://i.imgur.com/L1z5xG4.png)

그 다음으로 네트워크 아키텍쳐를 설계합니다. Hidden layer는 어떻게 할것인가 등등.

![51](https://i.imgur.com/zRLRDpN.png)

네트워크를 선택하고 코드를 구현하였다고 하면, 실제 구현한 네트워크가 잘 동작하는지를 확인해 봅니다.</br>
normal distribution을 이용하여 랜덤 값을 만들었고 이 데이터를 입력으로 넣으면, class가 10개라고 하였을 때,</br>
loss(cost)는 ln(10) = 2.3...이 됩니다. 균등하게 분포한 값을 넣었기 때문입니다.

![52](https://i.imgur.com/V4DhU2r.png)

ResNet 같은 경우에 어떻게 Training을 했는지 살펴보겠습니다.</br>
위의 자료와 같은 속성으로 설계를 하였고 특히, learning rate는 실험적으로 시도한 것을 확인할 수 있습니다.</br>
32k, 48k 반복 마다 10씩 나누는 방법을 사용하였습니다.

![53](https://i.imgur.com/N07msyN.png)

Learning rate의 크기에 따른 학습 변화를 한번 확인해 보겠습니다.</br>
만약에 Learning rate = 1e-6으로 상당히 작게 잡고 시작해 보면 loss가 줄어들지 않는 것을 볼 수 있습니다.</br>
즉, 학습이 제대로 안되고 있습니다.

![54](https://i.imgur.com/wRrzRFL.png)

반면에 Learning rate를 너무 크게 잡으면 어떻게 될까요?</br>
여기서는 Learning rate = 1e6으로 비정상적으로 큰 값을 주었습니다.</br>
이 때는 cost = NaN 값(Overshooting 되어 발산)이 됨을 알 수 있습니다.

![55](https://i.imgur.com/8QXVZEg.png) 

Hyper-parameter를 조정할 때에는 log - scale에서 조정하는 것이 좋습니다.</br>
그렇지 않으면 값이 너무 커서 비교 하기가 어려울 수 있습니다.

![56](https://i.imgur.com/PsmpIbE.png)

Hyper-parameter를 조정할 때, 네트워크 아키텍쳐 / learning rate / learning rate decay / regularization 등등을 사용자가 조절할 수 있고,</br> 이 값들을 tensorboard로 모니터링 하여 조정할 수 있습니다.

![57](https://i.imgur.com/jgo4xiE.png)

요약 하자면,

- Activation = Relu
- pre-processing으로 subtract mean / normalization등을 사용
- Xavier/He initialization 사용
- Batch Normalization을 사용
- Hyperparameter optimization 수행(log space에서 하는게 좋다.)