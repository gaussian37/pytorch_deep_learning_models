# 7. CNN #

※ 출처 : [https://www.youtube.com/playlist?list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5](https://www.youtube.com/playlist?list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5 "https://www.youtube.com/playlist?list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5")

※ 영상 : [링크](https://www.youtube.com/watch?v=rdTCxAM1I0I&index=6&t=603s&list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5)

※ 작성자 : [김진솔(gaussian37)](http://github.com/gaussian37)

![1](https://i.imgur.com/VypKI2L.png)

위와 같이 기본적이 32 x 32 x 3의 CIFAR-10 학습 데이터가 있다고 가정해 봅시다. 기본적으로 CNN은 3-dimension volume에서 동작을 하는데, 각각의 layer들은 volumes of activation 즉, activation이 적용된 volume을 입력 받아서 또 다른 volumes of activation을 출력합니다. 예를 들면, 5 x 5 x 3 filter가 있을 때, 이 filter를 image 위에서 convolution을 시킵니다. 
자료의 Convolve가 convolution operation의 정의가 됩니다. <span style="background-color: #FFFF00">공간적으로 이미지 내를 슬라이드 하면서 dot product 연산을 한다는 의미가 됩니다.</span><br>
연산을 하기 위해서는 input volume과 filter volume이 같아야 합니다. 위에서 보면 channel의 수가 같음을 알 수 있습니다.

![2](https://i.imgur.com/4QhGe8w.png)

filter는 일반적으로 left-top 부분 부터 right-bottom 부분까지 훑어 나갑니다. 하나의 filter 연산 결과만 보면 위의 예제에서, 5 x 5 x 3 만큼의 dot product 연산을 하고 결과로 1개의 scalar 값으로 리턴을 받습니다. 

![3](https://i.imgur.com/Y6wtnu9.png)

전체 image를 다 훑었을 때, filter 각각의 convolution 연산의 총 결과는 28 x 28 x 1이 됩니다. 이렇게 새로 생성된 matrix를 <span style="background-color: #FFFF00">Activation map</span> 이라고 합니다. 따라서 하나의 filter는 하나의 Activation map을 생성합니다. 

![4](https://i.imgur.com/zc8U5ib.png)

이번에는 녹색의 2번째 filter를 가정해 보겠습니다. 2번째 filter가 다시 한번 전체 image를 훑으면서 convolution 연산을 하게 되어 파란색 filter와 같이 동일한 크기(28 x 28 x 1)의 Activation map을 생성하게 됩니다.

![5](https://i.imgur.com/Jb6hgAc.png)

예를 들어 6개의 filter를 가지고 있다면, 5 x 5 x 3 filter를 6개 가지고 있다면 6개의 Activation map을 가지게 됩니다. 결국, 28 x 28 x 6의 결과물이 만들어 지고 이 결과물이 다음 layer의 input이 됩니다. 

![6](https://i.imgur.com/ruaLYtR.png)

정리하면, 32 x 32 x 3 이미지를 입력받았을 때, 6개의 5 x 5 x 3 필터로 convolution 연산을 하면, 28 x 28 x 6의 Activation volume을 얻게 되고 이것은 다음 layer의 input이 됩니다. 28 x 28 x 6의 새로운 인풋을 10개의 5 x 5 x 6 필터로 convolution 연산을 하면 24 x 24 x 10의 Activation volume을 얻게 됩니다. 

우리가 궁극적으로 <span style="background-color: #FFFF00">update해야 할 parameter는 filter의 값</span>들입니다. 
초기화는 일반적으로 random하게 시작하곤 합니다.

![7](https://i.imgur.com/H5L126A.jpg)

filter를 시각화 해서 접근해 보겠습니다.

![8](https://i.imgur.com/u250ljp.jpg)

첫 번째 convolution layer의 filter를 시각화 해서 보면 edge나 color등을 볼 수 있습니다. 이것들은 첫 번째 layer의 필터의 특징에 해당합니다. 

![9](https://i.imgur.com/cyUMJvq.jpg)

다음 단계를 확인해 보면 이전 단계의 특징들 조각 하나하나를 합쳐서 통합해서 볼 수 있습니다. 

![10](https://i.imgur.com/QBZ15W7.jpg)

더 깊은 level의 filter를 시각화 해서 보면 좀 더 상위 레벨의 이미지들을 볼 수 있습니다. 

![11](https://i.imgur.com/hAPDEWA.jpg)

관련 내용은 Hubel & Weisel 의 논문에 나오는 내용입니다. Convolution 연산은 topographical mapping에 해당하며 이 연산을 hierarchichy 한 구조로 만들면 high level에서 점점 복잡한 형상을 컨트롤 할 수 있다는 내용 입니다.

![12](https://i.imgur.com/TYySsmo.jpg)

위와 같이 5 x 5 filter가 32개 있다고 가정해 봅시다. one filter는 one activation map을 생성하므로 32개 Activations 이미지가 각각의 결과물을 시각화 한것에 해당합니다. 

![13](https://i.imgur.com/YQ5a00F.jpg)

Activation map을 살펴보면 이미지의 어떤 내용을 참조하고 있는지 알 수 있는데, 파란 박스의 Activation map은 원본 그림의 오렌지색 부분을 보고 있음을 추정할 수 있습니다. Activation map의 하얀색 부분이 Activation이 되는 부분(픽셀 값이 큰 부분)이 되겠습니다.

![14](https://i.imgur.com/SWF4lcf.jpg)

Convolutional 하다는 것은 2개의 signal인 image와 filter가 convolution 관계를 가지고 있다는 것입니다.   

![15](https://i.imgur.com/BTenevJ.png)

일반적으로 CNN은 위와 같이 구성됩니다. <br>
Conv & Relu / Conv & Relu & Pool / 마지막 FC 로 class의 점수를 계산합니다.<br>
위의 예를 보면 10개의 filer를 사용하여 10개의 Activation map을 생성한 것을 알 수 있습니다.

![16](https://i.imgur.com/mUYsA00.png) 

공간의 차원에 대하여 다시 한번 생각해 보면 32 x 32 x 3의 이미지에 5 x 5 x 3의 이미지를 convolution 해주었을 때, 28 x 28 x 1의 Activation map을 얻는다고 하였습니다. 이 연산에 대하여 좀 더 자세하게 알아보도록 하겠습니다. 

![16-1](https://i.imgur.com/e1cX4w6.gif)

![17](https://i.imgur.com/sJRiCnI.png)

7 x 7 input의 3 x 3 filter가 있다고 가정해 봅시다. 먼저 top-left에서 convolution 연산을 시작하게 됩니다. 이 때 이동 크기(stride)는 1로 가정합니다.

![18](https://i.imgur.com/7KXSWvh.png)

그 다음 한칸 right-bottom 방향으로 한칸 shift 하여 연산합니다. 

![19](https://i.imgur.com/bUgqcwb.png)

오른쪽으로 최대한 이동하면 5번 shift 할 수 있고 이것은 아래쪽 방향으로 이동할 때에도 같습니다. 따라서 5 x 5 output을 만들 수 있습니다. 다음으로 stride = 2로 설정하고 연산을 해보겠습니다.

![20](https://i.imgur.com/2GgtLa6.png)

처음 시작은 같습니다.

![21](https://i.imgur.com/9StvZK4.png)

2 칸만큼 오른쪽으로 이동하여 convolution 연산을 합니다.

![22](https://i.imgur.com/9A9VjBu.png)

따라서 3 x 3의 output을 얻게 됩니다. 

![23](https://i.imgur.com/AXSilM3.png)

stride = 3인 경우 입력 image와 사이즈가 맞지 않아 사용하지 않습니다.

![24](https://i.imgur.com/HHPsNvp.png)

일반화 해보면 output size = (N - F) / stride + 1로 표현할 수 있습니다.<br>
만약 사이즈가 맞지 않는 케이스를 사용해야 한다면, floor(버림) 처리를 하여 정수로 만들어서 사용할 수는 있습니다.

![25](https://i.imgur.com/fFZvqaY.png)

convolution 연산을 사용하면 image의 사이즈가 줄어들기 때문에 padding을 이용하여 사이즈를 유지합니다.  축소된 이미지와 원본 이미지 차이 만큼의 여백에 0의 값을 추가해주는 zero-padding을 주로 사용합니다. 
(N - F) / stride + 1 연산을 통하여 7 => 5로 변하였던 사이즈를 다시 7로 유지할 수 있습니다. 정확히 공식으로는 <span style="background-color: #FFFF00">(N - F + 2*P) / stride + 1</span>이 됩니다.

![26](https://i.imgur.com/zXmRkkE.png)

Zero-padding을 통하여 Size를 보존하면 사이즈에 신경쓰지 않게 해주기 때문에 편리하여 padding을 사용하는 이유가 됩니다.

![27](https://i.imgur.com/1ll19MG.png)

그러면 사이즈를 보존하기 위해서는 얼만큼 padding 하면 될까요? 일반적으로 (F-1)/2 크기만큼 padding 하면 됩니다. 
Convolution 연산 시 사이즈 관련 일반식을 정리하면 <br>
① N = (N+2P-F)/S + 1 <br>
② 사이즈 유지할 시 padding 사이즈 P = (F-1)/2<br>

![28](https://i.imgur.com/i4JMaqe.png)

padding을 이용하여 사이즈를 보존하는 것이 왜 중요한지 알아보겠습니다.<br>
앞의 예제를 보면 동일한 filter 사이즈를 지속 적용하면 32 x 32 x 3 → 28 x 28 x 6 → 24 x 24 x 10으로 channel수를 제외하면 height, width의 사이즈는 지속적으로 줄어들고 있습니다. 결국 layer가 깊어질수록 size가 0에 가까워 지게 되어 더 이상 layer를 stack 하는 것이 불가능해 집니다. 따라서 padding 작업을 통하여 이미지 사이즈를 유지해줄 수 있기 때문에 유용합니다. 

![29](https://i.imgur.com/Bp8aw06.png)

예제를 풀어 보면 32 x 32 x 3의 이미지를 10개의 5 x 5 필터를 사용하고 stride = 1, pad = 2를 사용한다면 N = (N+2P-F)/S + 1 에 따라 32 x 32 x 10의 사이즈를 가지는 output을 가지게 됩니다.

![30](https://i.imgur.com/C9oEWM9.png)

그러면 파라미터의 수는 몇개가 될까요? 각각의 filter에는 5 x 5 x 3 = 75개의 파라미터가 있습니다. 만약 bias가 있다면 +1을 해서 총 76개의 parameter가 존재하게 됩니다. filter의 갯수는 총 10개이므로 총 760개의 parameter를 가지게 됩니다. 

![31](https://i.imgur.com/XfkSvsh.png)

Convolution layer 총 정리를 위와 같습니다. 

![32](https://i.imgur.com/O9SBBXj.png)

① CNN 연산을 할때 4가지 hyper-parameter를 필요로 하게 됩니다.<br> 
- Filter의 갯수<br>
- Filter의 크기<br>
- stride<br>
- padding<br>
② output의 depth(D2)는 항상 Filter의 갯수(K) 와 같습니다.<br>
③ Parameter의 갯수 F * F * D1 * K (bias 없을 때)<br>

![33](https://i.imgur.com/aADr9Xg.png)

일반적으로 필터의 갯수는 2^N 형태로 사용합니다. 이렇게 사용하는 이유는 계산할 때 performance 측면에서 2^N 형태로 사용하는 것이 좋기 때문입니다.<br>
- Padding 사이즈 = (F-1)/2를 사용합니다.

![34](https://i.imgur.com/WE5vBzW.png)

1 x 1 filter & stride = 1을 사용하였을 때에는 어떤 의미가 있을까요?<br> 
만약 위의 입력 56 x 56 x 64가 2차원인 56 x 56 이라면 1 x 1 filter로 convolution 연산을 하면 아무 의미가 없을 수 있습니다.<br>
하지만 3차원에서는 depth에 대하여 dot product를 하기 때문에 depth를 조절할 수 있습니다. 따라서 depth를 조절해야할 때 1 x 1 filter를 이용하면 됩니다.<br>

![35](https://i.imgur.com/6MGLa2n.png)

뉴런의 관점에서 CNN을 좀 더 살펴보겠습니다.<br>
Image와 filter가 convolution 연산이 되는 영역을 receptive field라고 하는데 이 부분을 neuron으로 표현하면 오른쪽 박스와 같습니다.  이 부분들을 neuron으로 표현하여 summation을 하고 bias를 더해주는 것(Cell body) 이 convolution 연
산에 해당하는데 여기서 local connectivity 성질을 알 수 있습니다.

※ 박스의 용어는 아래를 참조하시기 바랍니다.

![35-1](https://i.imgur.com/FCJncqO.jpg)

Cell body(세포체) : 신경세포의 중심이 되는 부분으로 세포의 핵과 세포소기관이 있음<br>
Axon(축삭 돌기) : 전기적 신호를 전달하는 역할<br>
Dendrite(가지/수상 돌기) : 전기적 신호를 받아들이는 역할<br>
Synapse(시냅스) : 인접한 두 신경 사이의 구조로 전기적 신호를 화학적 신호로 변경하여 전송함<br>

![36](https://i.imgur.com/xAlMWXH.png)

1) 28 x 28 의 Activation map이 있을 때, map에 있는 각각의 <span style="background-color: #FFFF00">neuron은 모두 receptive field와 연결이 되어 있습니다.</span><br> 
2) <span style="background-color: #FFFF00">각각의 neuron들은 동일한 parameter들을 공유</span>합니다. Activation map 내의 각각의 neuron 들은 동일한 weight 즉, 동일한 parameter를 가지는 하나의 filter의 dot 연산의 결과이기 때문입니다. 이 성질을 parameter sharing이라고 합니다.

![37](https://i.imgur.com/92DvOFJ.png)

만약 filter의 수가 5개라고 한다면 depth가 5가 되고 conv layer의 output은 28 x 28 x 5를 가지게 됩니다.<span style="background-color: #FFFF00"> 각각의 Activation map에서의 동일한 위치에 있는 neuron들은</span> input 이미지의 동일한 곳을 바라보게 되지만 다른 filter를 사용하여 출력된 것으로 다른 weight들을 사용하게 되어 <span style="background-color: #FFFF00">parameter sharing이 발생하지 않습니다.</span>

<span style="background-color: #FFFF00">정리를 하면 동일한 depth 내의 neuron 들은 동일한 filter를 사용하므로 parameter sharing을 합니다. 하지만 별개의 Activation map에 속하는 동일한 위치의 neuron들은 input image의 같은 곳을 사용하였지만 filter가 다르므로 parameter sharing이 발생하지 않습니다.</span>

![38](https://i.imgur.com/HzufZju.png)

이제 까지 확인한 Convolution Layer에서는 사이즈를 계속 보존하였습니다. 이제 Pooling layer를 이용하여 사이즈를 조정하는 방법을 배워보겠습니다. 

![39](https://i.imgur.com/mHJHZ3Y.png)

Pooling layer를 이용하면 volume의 표현 방식을 좀더 작게 관리하도록 만들어 줍니다. 이 작업은 각각의 Activation map에 대하여 독립적으로 작용합니다.<br>
위의 예제를 보았을 때, 224 x 224 사이즈의 Activation map을 각각 112 x 112로 줄여주는 역할을 합니다. Activation map 각각에 독립적으로 사이즈를 줄이므로 depth는 그대로 유지가 됩니다. 2차원 관점에서 봤을 때는 down-sampling한 결과가 됩니다.<br>
<span style="background-color: #FFFF00">convolution layer에서는 사이즈를 유지하고 pooling layer에서는 사이즈를 down-sampling해주게 됩니다.</span><br>
pooling layer에서는 parameter가 없고 padding 또한 없습니다.

![40](https://i.imgur.com/AsoZ7lg.png)

pooling에 방법에는 크게 Max-pooling, Average-pooling이 있습니다. 주로 Max-pooling을 사용하게 됩니다. Max-pooling 시 pooling 사이즈에 따른 영역을 나눈 다음 각 영역에서 가장 큰 값을 가져오면 됩니다.<br>

Max-pooling을 하는 것이 정보 loss에 대한 우려가 있을 수 있습니다.<br>
하지만 의도적으로 Max-pooling 하는 것이 성능을 좋게 하는 경우가 있을 수 있습니다. 영역 중 가장 중요한 정보를 Max-pooling 하여 얻은 후 그 영역은 max-pooling 한 값만 사용하게 되면 invariance한 성질을 가지게 됩니다.<br>
예를 들어 자동차의 휠에 해당하는 부분이 정확히 매트릭스 어느 부분에 해당하는 지 알 수 없으나 가장 의미 있는 값을 max-pooling 하게 되어 사용하게 되어 휠이라는 것으로 사용하게 된다면, invariance한 성질을 얻을 수 있게 됩니다.

![41](https://i.imgur.com/kyJDSaV.png)

Pooling layer 또한 최종적으로 정리해 보면 W1, H1, D1을 입력 받아서 W2, H2, D2를 생성하게 됩니다. Convolution 연산과 사이즈 연산은 유사하지만 padding 연산은 없습니다. Depth는 변경이 없으므로 D1 = D2가 됩니다. <br>

<span style="background-color: #FFFF00">일반적으로 사용방법으로는 Filter size = 2, stride =2 입니다. 이 때 이미지의 사이즈를 1/2 로 할 수 있기 때문입니다.</span>

![42](https://i.imgur.com/jS52Oae.png)

마지막으로 FC에 대하여 알아보겠습니다. 10개의 filter가 있고, height,width = 4 일때,  마지막 pooling layer에서의 결과가 4 x 4 x 10이라고 하면, FC 를 위하여 1열로 펼쳤을 때, 160개의 수로 구성된 열 벡터를 만들 수 있습니다.<br>
이 후 FC를 구성하여 matrix multiplication 연산을 한 후 softmax 연산을 취하면 결과적으로 class를 구분할 수 있습니다.

![43](https://i.imgur.com/APqlltW.png)

다음으로 주요 CNN의 Case Study를 해보겠습니다. 처음 알아볼 Network는 LeNet-5 입니다. 위에서 C = Convolution, S = Subsampling(Pooling) 입니다. <br>
CONV-POOL-CONV-POOL-CONV-FC 로 구성되어 있고 filter = 5 x 5, stride = 1을 사용하였습니다. POOL 시 filter = 2 x 2, stride = 2를 사용하여 1/2 subsampling을 하였습니다.<br>

![44](https://i.imgur.com/R4OyZe1.png)

input으로  227 x 227 x 3 이미지를 받게 되고 (자료의 224는 오타입니다.)<br>
당시 GPU 성능의 한계로 2개로 병렬로 사용하여 위의 네트워크가 분리 되어 있습니다.<br>
현재는 GPU 성능이 지원하므로 하나로 합쳐서 구현하면 됩니다.<br>
96개의 filter = 11x11과 stride = 4를 사용하므로 First layer에서 conv 연산 시 사이즈는 55가 됩니다.<br>

![45](https://i.imgur.com/QkIbJa8.png) 

그러면 이 때 parameter의 수는 얼마가 될까요?

![46](https://i.imgur.com/Cc1ZcTX.png)

parameter는 filter와 관련이 있습니다. filter의 크기는 11x11x3 이고 filter의 수는 96이므로 (11x11x3)x96 = 35K가 됩니다. 

![47](https://i.imgur.com/ISS7rHW.png)

Pooling Layer 연산 시 output은 어떻게 될까요? filter = 3 x 3, stride = 2 이므로 (55-3)/2 + 1 = 27이 됩니다. depth는 유지가 되므로 27 x 27 x 96이 됩니다.

![48](https://i.imgur.com/AhF1seN.png)

파라미터의 갯수는 어떻게 될까요?

![49](https://i.imgur.com/aoMdpdZ.png)

<span style="background-color: #FFFF00">Pooling layer에서 발생되는 parameter의 갯수는 없습니다.</span>

![50](https://i.imgur.com/NH8C2hu.png)

AlexNet을 끝까지 표현하면 위와 같은 구조를 가짐을 알 수 있습니다.<br> 
여기서 1000개의 class로 나뉜 이유는 ImageNet을 사용한 Network이고 ImageNet의 클래스가 100개이기 때문입니다. NORM 형태는 현재 사용하고 있지 않는 내용입니다.<br>
AlexNet의 아이디어는 Layer가 깊어질수록 사이즈는 점점 작아지지만, Filter의 수는 점점 더 많아지는 것입니다.<br>

![51](https://i.imgur.com/uVQE8gd.png)

AlexNet에는 또한 최초로 Relu로 사용하였고 data augmentation을 많이 사용하였었습니다. dropout = 0.5, batch_size = 128 등 여러 optimization을 사용하였습니다.

![52](https://i.imgur.com/PYRo2Hc.png)

ZF Net에서는 AlexNet을 개선하여 성능 향상을 하였습니다.<br> 
1) CONV1 Filter 변경 : Filter = 11 x 11, stride = 4 → Filter = 7 x 7 , stride = 2<br>
2) CONV3, 4, 5 : Filter 수 변경 : 384, 384, 256 → 512, 1024, 512<br>
필터의 크기는 줄이고 필터의 수는 늘렸습니다.

![53](https://i.imgur.com/ZNkjogQ.png)

VGG net의 경우 <span style="background-color: #FFFF00">CONV에서는 오직 filter = 3 x 3, stride = 1, pad = 1만 사용</span>하였고, <span style="background-color: #FFFF00">POOL 에서는 filter = 2 x 2, stride = 2만 사용</span>하여 좋은 성능을 만들어 효과를 보았습니다. 
VGG net에서는 CONV, POOL 연산 방법은 같게 하나 layer 얼마나 쌓아야 performance가 최대가 되는지 고민을 하였고 16 layer를 쌓았을 때 최대가 되어 16-layer 방법을 사용하였습니다.
VGG-16에서 사용된 총 파라미터 갯수는 1.3억 개 입니다.

![54](https://i.imgur.com/iGyAfFx.jpg)

Layer를 전반적으로 분석해 보면 위와 같습니다. AlexNet과 마찬가지로 Image의 사이즈는 점점 줄어들게 되고 Filter의 갯수는 점점 늘어나게 됩니다. 사용된 메모리를 보면 24M에 각 float 변수가 4 byte 이므로 93MB / image 가 됩니다.<br> 
forward/backward propagation 을 생각하면 93*2MB가 각 이미지당 사용된 꼴입니다. 전체 파라미터의 갯수는 1.3억개 입니다.

메모리 전체 사용량은 CONV 연산에서 대부분을 차지하게 됩니다.(연산량)<br>
반면 파라미터의 수는 FC 연산에서 대부분 차지하게 됩니다. <br>

최근에는 FC 연산의 비효율성에 의문이 제기되어 Avg Pooling을 대체하여 쓰는 연구가 많이 진행되었습니다.<br>  
위에서 <span style="background-color: #FFFF00">7 x 7 x 512  를  FC로 4096으로 만들었습니다. 이 때 FC를 사용하지 말고 7x7 AVG-pooling을 사용하여 1 x 1 x 512를 사용</span>하였습니다.<br>
AVG-pooling을 사용하면 parameter 수를 대폭 줄여주기 때문에 아주 효율적이고 성능적인 측면에서도 FC와 유사합니다.

![55](https://i.imgur.com/pTc1GGt.png)

GoogLeNet에서는 기본적으로 Inception module이 사용됩니다. GoogLeNet은 상당히 복잡하기 때문에 2014년 2위인 VGG Net 보다 성능은 좋지만 성능 차이가 크게 없어서 보통 VGG Net을 많이 사용하곤 합니다.

![56](https://i.imgur.com/3SxV1Zn.png)

기본적인 구조는 위와 같습니다.

![57](https://i.imgur.com/rIU5BtG.png)

GoogLeNet에서 가장 주목할 점은 마지막에 사용된 avg pool 입니다.<br>
avg pooling layer에서 7 x 7 x 1024에서 avg pool을 적용하여 1 x 1 x 1024의 단일 column으로 줄인 점을 알 수 있습니다. <br>
따라서 FC layer를 완전히 제거함으로써 5M 정도의 parameter밖에 생성이 안됩니다.<br>
(VGG-16에서는 1.3억개의 parameter가 생성이 되었었습니다.)

![58](https://i.imgur.com/NWGahac.png)

최종적으로 Parameter 수를 비교해 보면 AlexNet = 60M, VGG = 138M, GoogleNet = 5M 입니다. AlexNet과 비료하면 parameter의 갯수는 1/12배로 줄이고 연산도 2배 빠르며 오답률 또한 10%이하로 줄였습니다.

![59](https://i.imgur.com/pRXuRLI.png)

다음으로 2015년 우승한 ResNet에 대하여 알아보겠습니다. ResNet은 모든 분야에서 1위를 석권하여 현재 가장 유명한 Net중 하나로 자리잡게 되었습니다.

![60](https://i.imgur.com/mDYccAP.png)

이미지 넷을 거치면서 Depth가 점점 증가함을 알 수 있습니다.<br>
ResNet에서는 무려 152 Layer를 구성하여 폭발적인 Layer 증가를 보인반면 Error율은 상당히 많이 개선되었음을 알 수 있습니다. <br>
기본적으로 Layer가 깊어지면 성능이 좋아지는게 맞지만 여러가지 문제 때문에 이전 까지는 성능이 좋아지지 않았었습니다. 

![61](https://i.imgur.com/KMDX9MT.png)

ResNet이전(왼쪽)은 layer가 많아질수록 Error rate가 점점 커졌습니다. 반면 RestNet은 layer가 많아질수록 Error rate가 줄어드는 이론과 현실이 일치하는 성과를 보였습니다.

![62](https://i.imgur.com/pRd5gQj.png)

Depth의 변화 추이를 보면 AlexNet = 8, VGG = 19, ResNet = 152임을 보았고 152 layer 구성시 8개의 GPU로 2~3주 정도의 학습 시간이 걸렸지만, run time 시 VGG Net 보다 빠른 성능을 보였습니다.

![63](https://i.imgur.com/9Wga6ch.png)

ResNet을 보면 CONV를 한번 거친 다음 바로 POOL을 거쳐 56 x 56으로 사이즈를 축소하여 효율적인 연산이 가능해지고 skip connection을 통하여 효율적으로 layer가 stack되는 구조가 됩니다.

![64](https://i.imgur.com/NgZNDji.png)

ResNet에서 사용되는 skip connection은 + 연산을 추가해서 back-propagation 시 distribution이 그대로 되도록 지원합니다. 즉, skip connection을 이용하여 이전 layer로 바로 넘어갈 수 있도록 합니다.

![65](https://i.imgur.com/xeQueEw.png)

즉, 가장 마지막 layer에서 순식간에 가장 처음 layer 까지 back-propagation 되도록 지원됩니다. 

![66](https://i.imgur.com/gCwET6E.png)

모든 CONV 에서 BN을 사용하였고, He initialization등을 사용하였습니다. 
BN을 사용하였기 때문에 AlexNet(Learing rate = 0.01) 보다는 큰 Learning rate = 0.1을 사용하였습니다. BN을 사용하면 Learning rate에 덜 민감해 지기 때문입니다. 
BN을 사용하였기 때문에 dropout을 사용하지 않아도 되기 때문입니다. 

![67](https://i.imgur.com/JfFKh7q.png)
![68](https://i.imgur.com/tZXTDct.png)

ResNet-50/101/152 또한 GoogLeNet과 유사하게 1 x 1 convolution을 사용하였습니다. 

![69](https://i.imgur.com/mI63V9Y.png)

위는 ResNet의 상세 사용 스펙 입니다.

![70](https://i.imgur.com/Kh9NTeb.png)

CONV에 대한 간략한 요약을 하자면 <br>
1) 최근 트렌드는 작은 필터를 사용하고 점점 더 깊은 아키텍처를 사용합니다.<br> 
2) POOL과 FC를 사용을 안하고 POOL 대신 CONV에서 stride로 조절 하는것이 최근 트렌드 입니다.<br>
3) 전통적인 내용은 학습하되 최근 트렌드인 ResNet과 GoogleNet을 따르는 것이 좋을것 같습니다.
