{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "930c1bc5-1df6-46a5-9218-a91bce5bc62b"
    }
   },
   "source": [
    "작성일 : 2018-01-10 19:56:05 <br>\n",
    "작성자 : Gauss Kim <br>\n",
    "참조 : [https://youtu.be/hVCuvMGOfyY](https://youtu.be/hVCuvMGOfyY) <br>\n",
    "참조 : [https://youtu.be/SC1wEW7TtKs](https://youtu.be/SC1wEW7TtKs) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9d711728-4909-4c58-821a-4c678720b845"
    }
   },
   "source": [
    "Feedforward는 입력을 출력으로 변환하는 데 사용되는 프로세스 신경 네트워크입니다.\n",
    "\n",
    "![1](http://postfiles8.naver.net/MjAxODAxMTBfMzIg/MDAxNTE1NTY5NjUzNDQ1.IvStIyLpQJ_HOiqUgFYI3WlRxlP7QkE_hsZvCqmCCbwg.Mmt9RctwQTNnWCRp1BVKG7Lw0Y1ue6vulJ8RoF5kypQg.PNG.infoefficien/32-1_Feedforward.mp4_000003143.png?type=w773)\n",
    "\n",
    "지금까지 Neural Network에 대하여 정의하였습니다. 이제 정의한 Neural Network를 통하여 어떻게 학습(Training)할 것인지 배워보도록 하겠습니다.<br>\n",
    "Neural Network를 Training하는 것은 <span class=\"mark\">어떤 Parameter를 Graph의 edge에 둘 것인지를 의미</span>합니다. <br>\n",
    "이 의미를 자세히 알기 위해서는 어떻게 input을 처리하여 output을 만드는지 알아야 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0e1998bb-4ff4-4184-9be4-3b21fe0b4e9d"
    }
   },
   "source": [
    "![2](http://postfiles8.naver.net/MjAxODAxMTBfMjI1/MDAxNTE1NTcwMTcyNzE1.YX50Zt5qhX-OrseqdakkuUtB5uBm5NKE7ksShz-t-c0g.cjmA7x9a-qUb3snnl2UMPTGfY2lVfeLoU_s6FZqFZEUg.PNG.infoefficien/32-1_Feedforward.mp4_000026979.png?type=w773)\n",
    "\n",
    "예를 들어 input $x = (x_{1}, x_{2})$이고 label $y = 1$이라고 가정해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a71e81e2-76e1-41d7-8416-292a9ef49ae5"
    }
   },
   "source": [
    "![3](http://postfiles2.naver.net/MjAxODAxMTBfMjcw/MDAxNTE1NTcwNDU2OTk5.0qu0GsK8jYWpRxYFCg4SOaC7jSzuClRW7lakk-3EHzcg.YtNwvYq5Fb-L0iSMW-TqW1gI7kLMA7hPEg4s1OQdVJMg.PNG.infoefficien/32-1_Feedforward.mp4_000066522.png?type=w773)\n",
    "\n",
    "앞의 강의와 연관해서 보면 $y = 1$이라는 뜻은 $x = (x_{1}, x_{2})$가 blue라는 뜻입니다.<br>\n",
    "$x_{1}*w_{1} + x_{2}*w_{2} + b $모델을 가지고 $w_{1}$의 크기가 크다고 가정하여 $w_{1}$ 간선을 좀 더 굵게 표현하였습니다. <br>\n",
    "이 때 위 모델은 사실 bad 합니다. 왜냐하면 blue point x가 red 영역에 있기 때문입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ff59d602-52d2-4944-be99-892d4a8e7490"
    }
   },
   "source": [
    "![4](http://postfiles7.naver.net/MjAxODAxMTBfMjU1/MDAxNTE1NTcwNjA1Mzk1.482ALHc3M3FitpYcBzkrj2cZKG1LHPeRgyHzcwIfWrsg.sm60ygA6nlI0oVSESpsvR2k3XrLtwPYM9AZul8RTIkEg.PNG.infoefficien/32-1_Feedforward.mp4_000123944.png?type=w773)\n",
    "\n",
    "만약 좀 더 복잡한 Neural Network를 구성하면 hidden layer 에서 각각의 모델에 맞게 분류가 될 것입니다.<br>\n",
    "이 때 output layer에서는 위의 모델에서 작은 output 값을 받고 아래 모델에서 큰 output 값을 받습니다.<br>\n",
    "위 모델은 <span class=\"mark\">오 분류되어 x 점의 P(blue)가 낮고 아래 모델은 잘 분류 되어 x점의 P(blue)값이 크기 때문</span>입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1a8df84d-8837-4fdd-afb1-302dc980d0c8"
    }
   },
   "source": [
    "![5](http://postfiles13.naver.net/MjAxODAxMTBfMTA0/MDAxNTE1NTcxMjg3MDg3.Ax4ybKe-gEi8M8oDsN9y6lW65TYSn16oN8a-p_v-LE4g.bLTioyrgpS-jdGZktGinF3vFlmMYAE8lYk51W5aJmm8g.PNG.infoefficien/32-1_Feedforward.mp4_000126743.png?type=w773)\n",
    "\n",
    "hidden layer를 추가하였지만, 아직 bad model 입니다. 왜냐하면 x가 오 분류 되어있기 때문입니다.<br>\n",
    "여기 까지 <span class=\"mark\">Input → Hidden → Output layer로 진행되는 것을 Feedforward</span>라고 합니다. Feedforward 연산을 어떻게 하는지 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "501be13c-021c-46e3-a541-1561d27d4734"
    }
   },
   "source": [
    "![6](http://postfiles16.naver.net/MjAxODAxMTBfNzIg/MDAxNTE1NTcyMjQ0Mzkz.QJfL_ozyPkHaAB5FESfWzxyTePxNMrn53h-J_Fx4diMg.x5hIU10YMAQptDSlOZxtPIv3WkH151fSRx07QUv2TuIg.PNG.infoefficien/32-1_Feedforward.mp4_000160514.png?type=w773)\n",
    "\n",
    "먼저 일반적인 Graph 표기 법으로 Perceptron을 표현해 보겠습니다.<br>\n",
    "Input layer의 $x_{1}, x_{2}$ 그리고 bias = 1로 표현하였습니다. weight는 layer l 및 i → j 인덱스 기준으로 $w^l_{ij}$로 표현하였습니다.<br> \n",
    "weight 표기를 풀어서 설명하면 layer는 l-1 → l 로 이동하고 (l-1) layer의 i번째 인덱스에 $w^l_{ij}$를 곱하여 l layer의 j번째 인덱스 노드에 전달하는 뜻입니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e70e16cb-9aef-491c-8644-f6686795cf01"
    }
   },
   "source": [
    "![6](http://postfiles9.naver.net/MjAxODAxMTBfNyAg/MDAxNTE1NTczNzg3NzY5._A635Y1xDZ1e7NF0_I-ac7X8Bh2ax8Chfe0xMQD8d78g.0qj9xY9DuLtPNhImfAJpvCVXJ56aLsQa_cTJYakSjUEg.PNG.infoefficien/32-1_Feedforward.mp4_000166737.png?type=w773)\n",
    "\n",
    "Input layer를 행렬로 표현하면 $(x_{1}, x_{2}, 1)$이 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0198b023-93d8-494e-b00b-e7ac9e37662d"
    }
   },
   "source": [
    "![7](http://postfiles1.naver.net/MjAxODAxMTBfMTk4/MDAxNTE1NTczODk4MTI4.9XcfQ_Tqgou0WZpqK7A9vET1KTP35qp46FdXen306z0g.ThrlKh4L1AuKCTt-GzZRwDE6SQ69gNopH8_uC1ntuFkg.PNG.infoefficien/32-1_Feedforward.mp4_000174628.png?type=w773)\n",
    "\n",
    "여기에 Weight를 곱하여 연산을 합니다. weight matrix는 같은 layer 단계끼리 묶어 (i, j) 에 맞춰서 구성합니다.<br>\n",
    "(위 표기는 실제 matrix multiplication을 나타낸 것은 아니고 간략하게 단계만 표현한 것으로 shape은 맞지 않습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "29fa4f50-4bc8-4012-b07f-d030f4abae7f"
    }
   },
   "source": [
    "![8](http://postfiles16.naver.net/MjAxODAxMTBfNSAg/MDAxNTE1NTczOTc3NjI1.nCO2_y97-XFn13Ir-lcoE6k4O1InaOP0YLmfHi_TXxwg.PS0bhy4wDa9RHSkIaqFqmJ0xb8KMb237vzlJV9TgA2Mg.PNG.infoefficien/32-1_Feedforward.mp4_000179086.png?type=w773)\n",
    "\n",
    "다음으로 sigmoid 연산을 적용합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6011e20d-e771-4bc0-81ed-e55a0007b092"
    }
   },
   "source": [
    "![9](http://postfiles3.naver.net/MjAxODAxMTBfMTA5/MDAxNTE1NTc0NTA2MjEx.ymDWQpoOINZcvM7hY6IBgvS_YJ28SpjUer4ouLpODdAg.fokwUVYkB_CjLBanXX5rsd6qt9vnl2vU4n4xxooveOkg.PNG.infoefficien/32-1_Feedforward.mp4_000197961.png?type=w773)\n",
    "\n",
    "전체 연산을 적용한 결과는 식을 참조하시기 바랍니다. 결과값 $ \\hat{y} $은 prediction이므로 입력값 x 점이 blue로 분류될 확률을 뜻합니다.<br>\n",
    "계산 과정을 차례대로 보면 Input vector를 받아서 linear model과 sigmoid를 차례로 적용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4972013a-257d-4b07-b704-067b8bb631b6"
    }
   },
   "source": [
    "![10](http://postfiles8.naver.net/MjAxODAxMTBfMTE4/MDAxNTE1NTc0Njk1NDQx.qDvLCv92Gb8XFb2baylqZh1gA_W3Lf1YDXPGFnHUIfIg.8zYoZA6kXfISCZlXCiCFa-PjXkzPcQ0FwNQVpGON8jIg.PNG.infoefficien/32-1_Feedforward.mp4_000219299.png?type=w773)\n",
    "\n",
    "이것을 간단하게 표현하면 $ \\hat{y} = σ · W^2 · σ · W^1 · x $입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "92c0f662-3526-42cb-946a-61321d755a46"
    }
   },
   "source": [
    "![11](http://postfiles1.naver.net/MjAxODAxMTBfNDQg/MDAxNTE1NTc1MDU5ODg3.0-Zz0_kmiiOPRZ1LS6pqzSRSdle-LRtrEvJYiSMP4dQg.C7yEm9E76lHCYjSEsRnME6tP9hefSIG1y-9MyeDdLk8g.PNG.infoefficien/32-1_Feedforward.mp4_000259503.png?type=w773)\n",
    "\n",
    "graph를 일반화 하여 표현하면 prediction $ \\hat{y} $를 구하는 Neural Network는 위와 같이 표현할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6cc1d154-52e3-44ea-810f-d06371e32464"
    }
   },
   "source": [
    "![12](http://postfiles4.naver.net/MjAxODAxMTBfMTM2/MDAxNTE1NTc1MjU4MDI3.-vHON1J1ck_Ra0FBwQFoCii6kZuMJWKo3mb-YOYg1jAg.uzZgR6QGj6GumE1VZljsLbKM1eIEnRkkvJOG0Ndiy6Ag.PNG.infoefficien/32-2_Feedforward.mp4_000006370.png?type=w773)\n",
    "\n",
    "다시 우리의 목적인 Network 학습을 고려해 보면 Error function을 정의 해야 합니다. <br>\n",
    "Error function은 Perceptron에 어떤 작용을 하는지 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "06c57afb-ee30-453d-8e3b-8fc5d40e41c7"
    }
   },
   "source": [
    "![13](http://postfiles4.naver.net/MjAxODAxMTBfMjY5/MDAxNTE1NTc1NDM4NjAz.RC3zXaF8COntINSm2kRRjKr5ZuvT8LjEsSvdWYYBVxkg.X183jToWZquZqJlsOzpr3PL4ef3yQkKsmDFQOQSlwFEg.PNG.infoefficien/32-2_Feedforward.mp4_000014852.png?type=w773)\n",
    "\n",
    "먼저 Input layer에는 Input vector $(x_{1}, ..., x_{n})$을 가지고 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6c3e049d-47b5-406c-8795-4928ea53a99d"
    }
   },
   "source": [
    "![13](http://postfiles14.naver.net/MjAxODAxMTBfMjEg/MDAxNTE1NTc1NDk4MzAy.27695HI0y7ymTJhbvQ9grObfcqX-dFLoXOv6DAIVY5gg.UB9R3lZ0wWYb1Go6H1mh4IPWKJz683tQzHvgvxhR-Jkg.PNG.infoefficien/32-2_Feedforward.mp4_000019055.png?type=w773)\n",
    "\n",
    "bias 를 가집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bd998469-9b8f-41db-afc4-7caaaea5eeca"
    }
   },
   "source": [
    "![14](http://postfiles1.naver.net/MjAxODAxMTBfMTU4/MDAxNTE1NTc1NTM3ODYz.EVtgj-dY1ant1-V8xkG_VRP_0KGMFJnYOAb4x0S6i1Eg.RTzLruFhnT7AbjEoQ_6X1_6OHFyhSTohF7zHM1PVqLgg.PNG.infoefficien/32-2_Feedforward.mp4_000021849.png?type=w773)\n",
    "\n",
    "Weight는 $w_{1}, ... , w_{n}$을 가지고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a03ba8e1-bd39-4aaa-844b-ca793a7e871e"
    }
   },
   "source": [
    "![15](http://postfiles14.naver.net/MjAxODAxMTBfMSAg/MDAxNTE1NTc1NTY1OTk3.YSZrCNLmO2WvGZDuayb6MckkXzfftayFn270SlwX848g.m-UQZb2Lj5jVJe4lAigzI_EYK9Y8hpUzFv8d30-vQTwg.PNG.infoefficien/32-2_Feedforward.mp4_000025523.png?type=w773)\n",
    "\n",
    "bias unit 을 가집니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "71b8b6bf-e5cb-41cf-a657-4873e0b21a14"
    }
   },
   "source": [
    "![17](http://postfiles16.naver.net/MjAxODAxMTBfMjk4/MDAxNTE1NTc1NjAwNDc1.kmkovH_5nswMwdncr9hxv9RVK-XmzGq1QE98TolLsMUg.IROA8-luIaIpXlZOiK06fRPcW2yqGaXTTgd418io3hkg.PNG.infoefficien/32-2_Feedforward.mp4_000029897.png?type=w773)\n",
    "\n",
    "그리고 이 Perceptron은 sigmoid function을 사용하였습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "046b5c0d-37eb-458e-ba53-8b10394cf0dc"
    }
   },
   "source": [
    "![18](http://postfiles16.naver.net/MjAxODAxMTBfMTkg/MDAxNTE1NTc1NjQ0ODEx.wETVj2JWu0B7hAGO9IpY27Stz93GHrIQIxLt1r6b6Fgg.M121LJJRkgz78lWC3I4NnDK6z8jo-BYvPLKny_46wBgg.PNG.infoefficien/32-2_Feedforward.mp4_000040955.png?type=w773)\n",
    "\n",
    "이것을 수식으로 표현하면 $ \\hat{y} = σ(Wx + b)$로 나타낼 수 있습니다. <br>\n",
    "그리고 앞에서 배운 Error function의 식은 위와 같이 표현할 수 있었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "584cee74-32c6-46d1-83bf-d8ae9c99175f"
    }
   },
   "source": [
    "![19](http://postfiles15.naver.net/MjAxODAxMTBfMjMw/MDAxNTE1NTc2MDgxMzQ3.kngh13Ry9N7fudkWvdBccUDWreCJTcdoSuEwIt59aVQg.U8V4kmBSa_YIDP8ovltEHr4Aus8v0MK5Ygi9I0WTD0sg.PNG.infoefficien/32-2_Feedforward.mp4_000044270.png?type=w773)\n",
    "\n",
    "이 함수는 각각의 점들이 얼마나 잘 분류 되었는지를 나타내는 수치가 됩니다. E(W)  값이 작으면 작을 수록 잘 분류된 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b17a1a45-0794-4b58-8c94-6c2374a3ea12"
    }
   },
   "source": [
    "![20](http://postfiles5.naver.net/MjAxODAxMTBfMTc3/MDAxNTE1NTc2MTA5OTM2.05bffClY5TLy1DxqDhB6MmlzYgk1HkBVv3fyorjYY3Mg.WZaNxFJoK_4L3qj1uBvjpn98oXXyu4FkgBB05h7-Xnog.PNG.infoefficien/32-2_Feedforward.mp4_000074817.png?type=w773)\n",
    "\n",
    "그러면 Multi-layer Perceptron에서 Error function은 어떻게 정의가 될까요? <br>\n",
    "일단 Prediction은 여러 층을 쌓은 형태로 변경이 됩니다. 하지만 Error function은 똑같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "687523ec-f51f-429c-91b0-23d3630c71d8"
    }
   },
   "source": [
    "![21](http://postfiles14.naver.net/MjAxODAxMTBfMTY0/MDAxNTE1NTc2MTA5OTQ2.7Rmvq-7Jj1S0NNWOwwZyGdgWU-R_GrHguDM4FFNcz9og.BApPfM_-CxjeGmboFFevIrD-AcYy2miwI-HhdZ71TvEg.PNG.infoefficien/32-2_Feedforward.mp4_000077819.png?type=w773)\n",
    "\n",
    "전체 model은 Multi-layer에서 더 복잡하여 Non-linear 성격이 강해졌지만 Error function은 동일하고 Error function의 의미 또한 얼마나 input data(점)들이 잘 분류 되었는지를 나타냅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "973662d7-a91e-4994-a0ff-afb05d1dbb9c"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {
    "59746d18-b9e2-411a-8c43-6ce558237915": {
     "id": "59746d18-b9e2-411a-8c43-6ce558237915",
     "layout": "manual",
     "prev": null,
     "regions": {
      "c5eb880b-8acb-482f-9e89-85a333d5154f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "29fa4f50-4bc8-4012-b07f-d030f4abae7f",
        "part": "source"
       },
       "id": "c5eb880b-8acb-482f-9e89-85a333d5154f"
      }
     }
    },
    "f2dfa9b2-63b0-48d8-ab66-9c26e445896f": {
     "id": "f2dfa9b2-63b0-48d8-ab66-9c26e445896f",
     "prev": "59746d18-b9e2-411a-8c43-6ce558237915",
     "regions": {
      "18632d10-c048-4b15-a69d-d51ecacb3844": {
       "attrs": {
        "height": 0.6,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9d711728-4909-4c58-821a-4c678720b845",
        "part": "whole"
       },
       "id": "18632d10-c048-4b15-a69d-d51ecacb3844"
      },
      "2efd557f-bbb7-4c2d-808f-821bee2a17f6": {
       "attrs": {
        "height": 0.2,
        "width": 0.4,
        "x": 0.1,
        "y": 0.7
       },
       "id": "2efd557f-bbb7-4c2d-808f-821bee2a17f6"
      },
      "44e27ff1-2c4f-4bb9-930d-c276abb381ca": {
       "attrs": {
        "height": 0.2,
        "width": 0.4,
        "x": 0.5,
        "y": 0.7
       },
       "id": "44e27ff1-2c4f-4bb9-930d-c276abb381ca"
      }
     }
    }
   },
   "themes": {}
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
