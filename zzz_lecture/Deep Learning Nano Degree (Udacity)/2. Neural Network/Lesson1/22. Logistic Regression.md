![1](http://postfiles8.naver.net/MjAxNzEyMjhfMzgg/MDAxNTE0NDQ0MDM0MDk3.FZS3GxfdqMlDblaBKBOBy8jm1rd8pxkmeMTfPs1nV4kg.-aEzbS3Nb2nua8ll95irBtI3Gt8Cb6nKWJxF6AffeyEg.PNG.infoefficien/22-1.Logistic_Regression.mp4_000005843.png?type=w773)

2가지 모델이 있다고 가정합시다. 학습한 바와 같이 왼쪽 모델은 나쁜 모델이고 오른쪽 모델은 좋은 모델입니다. 

![2](http://postfiles7.naver.net/MjAxNzEyMjhfNTQg/MDAxNTE0NDQ0MTU0MjMx.ovEFmy8b3Y9Mvdv2dAdOyRVM7XtfsqhOtZ8qK3hAP4Ug.iiOjhgWfZpvEWsSaTDtGTfTl8UYH2COjkCBw4sUcQrwg.PNG.infoefficien/22-1.Logistic_Regression.mp4_000023403.png?type=w773)

두 모델의 cross entropy를 구하기 위해 각각 포인트의 확률에 -ln 을 적용하였습니다. <br>
CE의 값이 오른쪽 모델이 더 작기 때문에 오른쪽 모델이 더 좋은 모델인 것을 알 수 있었습니다. 

![3](http://postfiles9.naver.net/MjAxNzEyMjhfNzcg/MDAxNTE0NDQ0Mjg5Mjg3.lGHcWgTOisOWQnrwBNEv4PsejV9wVjBVyHgm4wXOb04g.XaRqix_mDBIP7LKAh0qnYPUxJKwLdsiwbajySHhyWTIg.PNG.infoefficien/22-1.Logistic_Regression.mp4_000055743.png?type=w773)

Error function을 계산해 보도록 하겠습니다. 두 가지 케이스로 나눠 생각해 보려고 합니다. 첫 번째 케이스는 y = 1 즉, 파란색 점일 때 입니다. P(blue)일 확률을 y^로 표현해 보면 P(blue) = y^ 입니다. 파란색 영역에 있는 파란 점의 발생 확률이 빨간 영역에 있는 파란 점의 발생 확률 보다 더 큰 것을 알 수 있습니다.  <br>
Error function은 CE를 이용하여 구할 수 있습니다. 왜냐하면 Error function의 값이 작을 수록 좋은 모델이고 또한 CE의 값이 작을 수록 정확하게 분류한 좋은 모델이라는 공통점이 있기 때문입니다.

![4](http://blogfiles.naver.net/MjAxNzEyMjhfMzYg/MDAxNTE0NDQ0NjYzOTE0.fyexPcmUmStWEbOJp88i52kzYlgy2xy37AzHmArUEuIg.ETObyjdE4u3WqQncpLs5Y2HSkjby_JTUMo98m218l_8g.PNG.infoefficien/22-1.Logistic_Regression.mp4_000066867.png?type=w1)

Error function으로 CE를 적용하면 Error = -ln(y^) = [-ln(0.6), -ln(0.2)]가 됩니다.

![5](http://blogfiles.naver.net/MjAxNzEyMjhfMTcx/MDAxNTE0NDQ1Njc4ODEy.ITjwNDvIVKYT5C1LWrmBucR1coPHDF0-X-pOFJNfb3Eg.PD4kM3Bd2aFceQrH4OEZjm1aKtmYz_FF6bla7k5WJgkg.PNG.infoefficien/22-1.Logistic_Regression.mp4_000101698.png?type=w1)

두 번째 케이스인 y = 0을 보면 P(red) = 1 - P(blue) = 1 - y^이 됩니다.<br>
Error = -ln(1 - y^)이 됩니다.

![6](http://blogfiles.naver.net/MjAxNzEyMjhfMjgy/MDAxNTE0NDQ1ODEwMjYy.oD1KpjNO9aVqe4y7tXU28-Tvposxiccdv0UnDMleSzIg.wfW6YnRu2oLPOI2MhW4WHN15hMzIl0bn8CrfacqXMMAg.PNG.infoefficien/22-1.Logistic_Regression.mp4_000114381.png?type=w1)

### 따라서 binary-class의 Error = -yln(y^) -(1-y)ln(1-y^)으로 표현할 수 있습니다. ###

![7](http://blogfiles.naver.net/MjAxNzEyMjhfMjU4/MDAxNTE0NDQ2MDk5MTA3.jOrOFeEtjQSISQ0KwkA6JhKlS_iF67fwonbk-6tXU-kg.L6KSN0xZLWWZ52f0pZzBo_YQgCCfLLu8fezKliVg8UEg.PNG.infoefficien/22-1.Logistic_Regression.mp4_000129188.png?type=w1)

Error = -yln(y^) -(1-y)ln(1-y^) 이 작동하는 이유는 binary class에서 y = 1 인 경우 (1-y) = 0 이 되어 (1-y)ln(1-y^) = 0이 되기 때문이고,

![8](http://blogfiles.naver.net/MjAxNzEyMjhfMjEz/MDAxNTE0NDQ2MTYzMjY5.q1x4VS_vuRU_SE7oXWnWbAzFiuG7oYyoHuNtsTUebYcg.AmKc9272aGccWWCHjU1muMf8PubyyytOTA930MmCHbog.PNG.infoefficien/22-1.Logistic_Regression.mp4_000139759.png?type=w1)

반대로 y = 0 인경우 yln(y^) = 0이 되기 때문입니다.

![9](http://blogfiles.naver.net/MjAxNzEyMjhfMjY3/MDAxNTE0NDQ2MjI1NDg1.RufvRzrGE4d3p8ZogJ4a0bR4LyuWu8YrKA34XzGms1cg.r706Aqkj_-LjzFMIUL-OCXxqwHeiKHzdDgmguJ4mYWMg.PNG.infoefficien/22-1.Logistic_Regression.mp4_000153471.png?type=w1)

Error function의 최종 formula는 위와 같이 표현할 수 있습니다. 위의 예제의 CE = 4.8입니다. 여기서 단순히 summation이 아닌 average 개념을 적용하여 point의 갯수를 나눠보겠습니다. 따라서 CE = 4.8 / 4 = 1.2가 됩니다. 

자세히 살펴보면 다음과 같습니다.<br>
![9-1](https://i.imgur.com/Afe2ArW.png)

![9-2](https://i.imgur.com/RQu1qJE.png)


![10](http://blogfiles.naver.net/MjAxNzEyMjhfMzgg/MDAxNTE0NDQ2Njc1MzI3.e16gn_U6EpI6Y_09CKCdHd-bwJhr26_sD9ShL3r4zu0g.cYm0lIzodQy-qqDFnJEsWNN41Vo3RoEQAg1WW1Ir_wAg.PNG.infoefficien/22-1.Logistic_Regression.mp4_000175236.png?type=w1)

여기서 사용될 y^ = sigmoid(Wx+b) 입니다. 식을 좀더 자세하게 표현해 보겠습니다.

![11](http://blogfiles.naver.net/MjAxNzEyMjhfMjM0/MDAxNTE0NDQ2ODQ5MDEx.tJBzFY2EJSyQFOUAObsqIxoED8l3oKy9yVFN0PHkqyIg.fGCCI9TLBkp-96HKixpOM3HObufzvAaDhnSZiv0K37Eg.PNG.infoefficien/22-1.Logistic_Regression.mp4_000187880.png?type=w1)

Binary class logistic regression일 때 Error function에 사용되는 variable은 결국 Weight와 bias입니다. 따라서 Error function = E(W, b)로 표현할 수 있습니다. 

![12](http://blogfiles.naver.net/MjAxNzEyMjhfMTYw/MDAxNTE0NDQ3MDAzNDM5.crKyNoKmgLhnSbukzQaV0cff_uMfWsNxdIAsb5-xCYMg.bkxojW0FpaO3hH2pvfDaqwffqvc4_ANHLTpfocpfc6Ig.PNG.infoefficien/22-1.Logistic_Regression.mp4_000206281.png?type=w1)

지금 까지 살펴 본 내용은 binary class - logistic regression의 Error function이었습니다. 유사하게 multi class - logistic regression의 Error function 또한 적용해 볼 수 있습니다. 

![13](http://blogfiles.naver.net/MjAxNzEyMjhfMTMw/MDAxNTE0NDQ3NjM5MjAx.3ZT0O6FFFylu-DlowsdkLGJxI8-Uv4lYQKPc2ckoc4kg.39FzMZ_sf8ehSyw5VG0ylfJPR1RlQQqeMu9RKHpcJB0g.PNG.infoefficien/22-2.Logistic_Regression.mp4_000011585.png?type=w1)

이제 Error function 적용을 자세히 살펴 보면 우리의 목표는 먼저 Error function을 최소화 하는 것입니다. 먼저 시작은 random weight로 하여 sigmoid(Wx+b) 모델을 가집니다. 

![14](http://blogfiles.naver.net/MjAxNzEyMjhfMTQ0/MDAxNTE0NDQ4MDQzNTQx.nK30VdEJJ00bffYwpV5YDVzveCuQwENlu0vZVWx1FzIg.G-wZqKPgpDN7zrcG6fr2LzEAwC2lT6qdtzg0U-OjxzUg.PNG.infoefficien/22-2.Logistic_Regression.mp4_000041767.png?type=w1)

E(W, b) 을 minimize 하는 것이 우리의 목표 입니다. 현재 오 분류되어 있는 포인트가 2개 있습니다. Error를 줄여보도록 합시다.

![15](http://blogfiles.naver.net/MjAxNzEyMjhfMTU0/MDAxNTE0NDQ4MDc5ODk2.FmBoj_J0sw4TJ_8QZBGNXUWrqgcyMlst3-GByyIe220g.h1M7Spp0kH5Nm7-f1KuOgTNK-DC4erAoQKydUcitJN0g.PNG.infoefficien/22-2.Logistic_Regression.mp4_000066406.png?type=w1)

E(W, b)를 줄여서 E(W', b')으로 만들면 모든 point들을 정확하게 분류할 수 있습니다. 그러면 어떻게 E(W, b) → E(W', b')으로 줄이는 지 다음 강의에서 부터 알아보도록 하겠습니다.