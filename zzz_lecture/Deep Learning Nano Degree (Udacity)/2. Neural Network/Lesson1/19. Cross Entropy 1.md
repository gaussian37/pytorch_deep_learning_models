![1](http://postfiles13.naver.net/MjAxNzEyMjhfMTYy/MDAxNTE0NDEyODk1OTc0.x3zwcAFKOqnCzRW_yKjOq4P0SWdnKwmJ7jIoAb9tXPcg.mCbp-10o8p71gTppF5nLA8Evd_8vuIubCriBGK3Hb74g.PNG.infoefficien/19._Cross_Entropy1.mp4_000003873.png?type=w773)

확률 곱을 합으로 바꾸려면 log 연산을 취하면 됩니다.  log(ab) = log(a) + log(b)가 되기 때문입니다. <br>
따라서 앞으로 곱셈 연산을 받으면 log 연산을 취할 것입니다.  

![2](http://postfiles13.naver.net/MjAxNzEyMjhfMTUz/MDAxNTE0NDEzMDI3Njkx.MKHlTC90Jhe7fGN8mqiMimuOg99981h2HhC0drovce8g._LwtCbAhynHr2lI2tN2dXgZkB2kgSfsSCPmPArrpMg8g.PNG.infoefficien/19._Cross_Entropy1.mp4_000101333.png?type=w773)

곱셈을 입력 받은 후 log를 취할 때 사용할 log 함수는 base가 exp인 natural logarithm 을 사용합니다. 밑이 10인 log를 사용하여도 상관은 없습니다.<br>
그리고 확률 값들은 항상 [0, 1] 범위를 가지기 때문에 log를 취하면 항상 음수가 나오게 됩니다. 확률값이 최대인 1일 때 log 값은 0이 나오게 됩니다.<br>
따라서  <font color='red'>log 값에 -를 붙여서 모두 양수 값</font>으로 만들 수 있습니다. 이 값을 cross entropy라고 합니다. 즉, 확률값에 -log를 취한 값들을 모두 더한 것입니다.<br>
위의 자료에서 왼쪽의 확률 곱은 0.0084이고 오른쪽 확률곱은 0.3024로 오른쪽 확률이 더 높아 더 좋은 모델이라고 여겼습니다.
반대로 cross entropy에서는 왼쪽의 값이 4.8이고 오른쪽의 값이 1.2로 오른쪽 모델의 cross entropy가 더 작은 값을 가집니다. <br>
즉, <font color='red'>좋은 model은 cross entropy가 낮고 나쁜 model은 cross entropy가 높습니다.</font>

![3](http://postfiles4.naver.net/MjAxNzEyMjhfMTQ2/MDAxNTE0NDE1NjUxMzQy.Z0uF1YEfmQ8nIq_8dmsJBpuPKaBaPCaq-a4qyLAQyBog.vDHmbplptbryy0aCcUa_qjs8DUjFKNESuCR5s5cOXt4g.PNG.infoefficien/19._Cross_Entropy1.mp4_000187608.png?type=w773)

cross entropy 방법은 생각보다 효과적입니다. cross entropy를 이용하면 각각의 point 에 대한 error를 구할 수 있습니다.<br> 
먼저, 두 model 각 point의 확률에 대하여 다시 알아보도록 하겠습니다. 각각의 확률에 cross entropy를 적용하면 즉, -log를 적용해 보면 위 자료 포인트 각각의 양수 값과 같습니다. 양수 값들을 비교해 보면 빨간 영역의 빨간 점, 파란 영역의 파란 점과 같이 잘 분류된 포인트 들은 양수 값이 작은 반면 오 분류된 점들은 양수 값이 큽니다.<br>
잘 분류된 포인트 들은 확률 값이 1에 가까워 지므로 -log 값은 오히려 0에 가까워 집니다. <br> 
따라서 <font color='red'>-log 값은 error와 동일한 상관관계</font>를 가집니다. 잘 분류된 point는 0에 가까운 -log 값을 가지고 error function의 결과 또한 0에 가까워 집니다. 반면 오 분류된 point는 무한히 커지게 되고 error function의 결과 또한 무한히 커지게 됩니다.<br>
그러므로 <font color='red'>cross entropy는 model의 좋고 나쁨을 판단할 수 있는 척도가 됩니다.</font><br>
앞으로 좋은 <font color='red'>model을 선정하는 기준은 높은 확률을 얻는 것 → cross entropy 값을 낮추는 것으로 변경</font>하겠습니다. 