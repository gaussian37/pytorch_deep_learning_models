참조 : [https://youtu.be/uL5LuRPivTA](https://youtu.be/uL5LuRPivTA)

Gradient Descent 알고리즘과 Perceptron 알고리즘을 비교해 보겠습니다.

![1](http://postfiles4.naver.net/MjAxODAxMDlfMTM1/MDAxNTE1NDcyNjI0Mzc3.nn5gP34ZSxodJjsW450_iB6LyLVBsxZvBKzJiMmUoHYg.Cd_PDp90EaUfONCS-wTLJvBZ-oHaeFFBL53X5zhuhDcg.PNG.infoefficien/27._Gradient_Descent_Vs_Perceptron_Algorithm.mp4_000076079.png?type=w773)

Gradient Descent 알고리즘에서 activation으로 sigmoid를 사용하면<br>
모든 x_i에 대하여 **w_i ← w_i + α*(y-y^)x_i** 로 업데이트가 되었습니다.<br>
**Perceptron 알고리즘**에서는 모든 포인트들의 weight를 변경하지는 않습니다. 오직 **오 분류된 점들만 weight를 변경**합니다.
오분류된 점들에 한하여 weight를 ±α*x_i를 weight에 연산합니다.<br>
(+α*x_i 연산은 label = 1, prediction = 0으로 오분류 되었을 때, -α*x_i 연산은 label = 0, prediction = 1으로 오분류 되었을때임)<br>
Perceptron 알고리즘에서는 label y = 0 / 1의 값을 가지고 prediction 또한 y^ = 0 / 1의 값을 가집니다.<br>
따라서 정 분류 된 케이스는 y - y^ = 0의 값을 가지고 오분류 되었을 때에는 y - y^ = 1 / -1의 값을 가집니다.<br>
(label = 1, prediction  = 0 일 때 1을, label = 0, prediction = 1일때 -1을 가짐)

왼쪽의 gradient descent와 오른쪽의 perceptron은 사실 동일한 내용입니다.<br>
차이점은 gradient descent의 경우 y^ = [0, 1] 값을 가지고, perceptron의 경우 y^ = 0 / 1의 값을 가집니다.

![2](http://postfiles14.naver.net/MjAxODAxMDlfMTQ0/MDAxNTE1NDczNjUxNjMx.hNlyEo5FIjIcg4B-FAIchx39V8_Imjdb4wT_O5pixU4g.OY7x61KXSuc8Zv3KVgSuk5bh1oygG0TqTOVZcT_E7lgg.PNG.infoefficien/27._Gradient_Descent_Vs_Perceptron_Algorithm.mp4_000144144.png?type=w773)

Gradient Descent 과 Perceptron 알고리즘 둘 다 오 분류된 점들은 classify line이 좀 더 오 분류된 점들에 가까이 오길 원합니다. 그래서 오 분류된 점들은 이 line이 오분류 점들을 통과하여 정 분류된 영역으로 분류되기를 원합니다. <br>
반면에 정 분류된 점들은 어떻게 반응 할까요?<br>
먼저 Perceptron 알고리즘에서는 어떠한 작업도 일어나지 않습니다. <br>
Gradient Descent 알고리즘에서는 weight를 업데이트하여 정 분류된 포인트 입장에서 line이 정 분류된 점들과 더 멀어지기를 원합니다.<br>
즉, blue 영역의 blue 포인트는 점점 더 blue 영역 깊은 곳으로 들어가길 원하게 됩니다. 

이것을 앞에서 배워왔던 개념과 접목시키면 정 분류된 점의 prediction도 점점 더 1에 가까워 지려고 합니다. 이것은 점점 더 정 분류된 영역의 안쪽으로 들어가려는 것과 같고 오 분류된 점의 prediction도 점점 증가하여 1에 가까워 지려고 할 것입니다. 이것은 분류 line을 점점 더 자기 자신쪽으로 가져와서 영역 안으로 들어오려고 하는 것에 해당합니다. 