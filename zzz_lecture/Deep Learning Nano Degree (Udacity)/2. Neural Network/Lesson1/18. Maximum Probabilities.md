이전 강의 Maximum Likelihood를 보았을 때 더 좋은 model은 더 나은 확률을 가집니다. 그러면 어떻게 확률을 극대화 할까요?<br>
이전 기억을 상기시켜 본다면 error function을 minimizing 하는 것이 최적 해를 구하는 것임을 알 수 있었습니다.<br>
error function과 Maximum Likelihood를 연결해 볼까요? error function에서 확률을 얻을 수 있을까요? 그러면 확률을 최대로 하는 것과 error function을 최소화 하는 것은 같은 의미를 가질 수 있습니다. 

![1](http://postfiles13.naver.net/MjAxNzEyMjdfMTAz/MDAxNTE0MzgzMTQ5NjI5.hJvCzdqQ4RA6KsDDS34K8tnbgY7Ftmtm4oqgyNlAB8Yg.TEBfFwI5Ca_Wcv3zA7q5Iz7LJSIi-RIqvHPZk7WwyzUg.PNG.infoefficien/18-2_Maximum_Probabilities.mp4_000000000.png?type=w773)

2가지 model이 있다고 가정합시다. 오른쪽 model은 각 포인트가 잘 분리되어 있어 왼쪽 model 보다 좋습니다. model의 좋고 나쁨을 판정하는 것은 각각의 point 들의 확률을 계산하여 확인할 수 있습니다. 전체 확률을 얻기 위하여 포인트 각각의 확률을 곱하면 오른쪽 모델의 전체 확률이 왼쪽 보다 더 큰 것을 알 수 있습니다. (0.3024 > 0.0084)

![2](http://blogfiles.naver.net/MjAxNzEyMjdfMTY3/MDAxNTE0MzgzOTQ5MzQ3.AzZ7MUEySD-QU0SAd1LjC1vuDVrw0FBd7lLVJDZOjJkg.tckFZQg-6yNQTPDELZ9IjTdHxqHherTtwh4J9zGCHUcg.PNG.infoefficien/18-2_Maximum_Probabilities.mp4_000091189.png?type=w1)

우리가 해야할 것은 이 확률을 최대로 해야 하는 것입니다. 이 때 확률은 포인트 각각의 확률을 곱하는 것입니다. 하지만 확률값을 계속 곱면 숫자가 매우 작아집니다.<br>
따라서 가능한 한 확률 곱 연산을 지양하고 확률 곱 대신 확률 합 연산을 해보려고 합니다. <br> 
곱을 합으로 바꾸려면 log 연산을 취해야 합니다.