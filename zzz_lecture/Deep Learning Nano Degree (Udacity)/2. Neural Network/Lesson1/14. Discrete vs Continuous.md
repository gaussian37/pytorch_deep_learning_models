지난 강의에서 Discrete와 Continuous 한 error function의 차이를 배웠습니다. </br>
그리고 실제 **gradient descent 시 사용해야 하는 것은 Continuous한 error function** 이었습니다.</br>
따라서 **discrete prediction을 continuous prediction으로 이동하는 방법**에 대하여 알아 보려고 합니다.</br>

![1](https://i.imgur.com/ndEWTKv.png)

Discrete Prediction에는 Yes/No와 같은 답이 있으나 Continuous Prediction에는 확률로 나타낼 수 있습니다.

![2](https://i.imgur.com/zbzK2CI.png)

어떤 특정 알고리즘을 통해 학생이 accepting인지 reject 인지 판단한다고 가정합시다.</br>
Discrete에서 0을 reject(red), 1을 accept(blue)라고 합시다. </br>
반면에 Continuous에는 확률로 나타납니다. 파란색 영역에 있는 점들은 높은 확률을 가집니다.</br>
예를 들어 파란색 영역에 있는 점 중 하나는 85%의 확률로 파란색 점입니다. 반대로 빨간색 영역에 있는 점들은 매우 낮은 확률을 가집니다. 이 영역에 있는 점 중 하나는 20%의 확률로 파란색 점이 됩니다. 

![3](https://i.imgur.com/CNIDrd6.png)

Discrete한 prediction을 Continuous하게 바꾸는 방법은 step function을 sigmoid function으로 바꾸는 것입니다.</br>
sigmoid function은 입력값이 큰 양의 값은 1에 가깝게 큰 음의 값은 0에 가깝게 만드는 function입니다. 그리고 입력 값이 0에 가까우면 결과 값은 0.5에 가까워집니다. 

![4](https://i.imgur.com/bhrruoq.png)

사용할 모델이 positive/negative 영역으로 구분되어 있다고 가정합시다. 각각의 point들을 각 영역에 분포가 되어 있습니다. 그리고 각 point의 확률이 1에 가까우면 파란 영역에 0에 가까우면 빨간 영역에 분포합니다.

![5](https://i.imgur.com/QvRy6o4.png)

위의 Point는 P(BLUE) = P(RED) = 0.5의 확률 값을 가집니다. 

![6](https://i.imgur.com/tsr4XpB.png)

위의 Point는 P(BLUE) = 0.4, P(RED) = 0.6의 값을 가집니다. 

![7](https://i.imgur.com/g0d7Z11.png)

위의 Point는 P(BLUE) = 0.3, P(RED) = 0.7을 가집니다. 

![8](https://i.imgur.com/KzMbvBd.png)

위 공간에서 확률을 얻는 방법은 간단합니다. sigmoid와 Wx + b를 통하여 얻습니다.</br>
먼저 Wx + b의 값에 대한 분포가 위와 같다고 가정합시다.

![9](https://i.imgur.com/qtVyPS9.png)

왼쪽과 같이 Wx + b의 분포에 따라 큰 값을 가지면 파란색, 작은 값을 가지면 빨간색에 가까워 집니다.</br>
이 때 Wx + b의 값에 sigmoid를 취하면 값이 커지면 커질수록 1에 가까워지고(blue) 값이 작아지면 작아질수록 0에 가까워집니다.(red)</br>
또한 Wx + b = 0일 때의 sigmoid 값은 0.5임을 알 수 있습니다.

![10](https://i.imgur.com/T5KaWdy.png)  

왼쪽 perceptrion은 activation function으로 step function을 사용한 것이고 오른쪽은 sigmoid function을 사용한 것입니다.</br>
input 값에 weight를 곱하고 bias를 더하여 activation function을 적용하는데 step function에서는 discrete한 0/1 값을 반환하였고 sigmoid function에서는 continuous한 [0, 1] 확률 값을 반환합니다.