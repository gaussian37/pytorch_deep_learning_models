Event와 확률 집합이 있을 때, 각각의 확률에 따라서 어떻게 event 들이 발생을 하게 될까요? 이전 내용을 기억해 보면 발생 확률이 높다면 작은 cross entropy를 가지게 됩니다. 반면 발생 확률이 낮다면 높은 cross entropy를 가지게 됩니다.   

![1](http://blogfiles.naver.net/MjAxNzEyMjhfNjkg/MDAxNTE0NDI1NTE2MDU3.wFlM3bkqRY0wU-_0oJKeNeol2XWVu0Tev5jhkgf4nfcg.EaCMR7a8yUu_Tzk_NHDMSc8eVDLhUDnLYdqGjCvV_Iwg.PNG.infoefficien/20-2._Cross_Entropy_2.mp4_000032409.png?type=w1)

초록색 문 뒤에는 80% 확률로 선물이 있고 빨간색 문 뒤에는 70% 확률로 선물이 있고 파란색 문 뒤에는 10% 확률로 선물이 있습니다.  반대로 생각하면 각각 20%, 30%, 90 % 확률로 선물이 없습니다. 

![2](http://blogfiles.naver.net/MjAxNzEyMjhfMjk1/MDAxNTE0NDI1NjA0MDI1._99BuM3ccO2sxi4VdvrVFRaSJns0M57ivTVwe_-CCDMg.5dLoF2fxQF1AZO4dXTlM5O-tbIAR5B3SjBfaGzcawFMg.PNG.infoefficien/20-2._Cross_Entropy_2.mp4_000091262.png?type=w1)

선물을 받을 확률을 1행, 받지 못할 확률을 2행에 정리하였습니다. <br>
위 확률을 기반으로 생각하였을 때, 가장 일어날 것 같은 케이스는 어떤 것일까요? <br> 
가장 발생할 수 있는 케이스는 각 열에서 높은 확률을 선택하는 것입니다. 따라서, 0.8 x 0.7 x 0.9 = 0.504가 가장 발생할 가능성이 있는 케이스의 확률 입니다.

![3](http://blogfiles.naver.net/MjAxNzEyMjhfMTM4/MDAxNTE0NDI1ODMxNjgx.dTezgJ1dlTmJngHIAV7ITztyJ4ia4UvUtmgpg8hUCxwg.2fKFMHuPbXj_M_D39xBI5vdQM6arxxmqv0AuNSfkhh8g.PNG.infoefficien/20-2._Cross_Entropy_2.mp4_000144144.png?type=w1)

전체 케이스(gift / no gift)를 고려해보면 총 8가지로 분류할 수 있습니다.<br>
각각의 확률을 곱하여 probability를 계산해 보고 Cross Entropy또한 계산하여 정리해 보았습니다. <br>
Cross Entropy의 의미를 다시 상기 시켜 보면 낮은 확률은 높은 cross entropy를 가지고 높은 확률은 낮은 cross entropy를 가집니다. <br>
예를 들어 가장 발생 확률이 높은 2번째 케이스인 0.504 확률은 cross entropy로 표현하면 가장 작은 0.69값을 가집니다.  반면에 가장 확률이 낮은 7번째 케이스인 0.006 확률은 cross entropy로 표현하면 가장 큰 5.12 값을 가집니다. <br>

![4](http://blogfiles.naver.net/MjAxNzEyMjhfMTQ2/MDAxNTE0NDI2MTM2ODQz.ULUzCxoVRSKz4G7Llaij5DQU-mavy-rogs3jtddstlsg.ujaXAZn9WTYYMggTZkEpEp5dha7v1Wp6erLTdho0O1og.PNG.infoefficien/20-2._Cross_Entropy_2.mp4_000320320.png?type=w1)

가장 발생할 가능성이 높은 케이스를 기준으로 Cross Entropy를 정리해률 보겠습니다.<br> 
초록색 문 = gift, 빨간색 문 = gift, 파란색 문 = no gift가 가장 발생할 확률이 높고 이 때 확률은 0.8 / 0.7 / 0.9 였습니다. 이 때의 cross entropy를 구하면 -ln(0.8) -ln(0.7)-ln(0.9)입니다. <br>
이 때 y_i = i번째 문에 선물이 있으면 1을 가지고 그렇지 않으면 0을 가진다고 가정합시다. 그러면 위의 가장 발생 확률이 높은 케이스는 y1 = 1, y2 = 1, y3 = 0 입니다.

![5](http://blogfiles.naver.net/MjAxNzEyMjhfMTMz/MDAxNTE0NDI3MDY4NTY4.5d9vdjENqp5sTvKbYb7Wcg46kUF-jwAOmyvCZ3rs_qkg.uqnptQvlxEBMtEOSwnK1AzPF7t2KzPR_TG68JSv0qlYg.PNG.infoefficien/image.png?type=w1)

이 때 Cross Entropy 식을 전개하면 위와 같습니다. <br>
만약 y_i = 1이라면 (1-y_1) = 0이되어 Σ 내부의 첫번째 term은 살아남고 두번째 term은 0이됩니다. 반면에 y_i = 0이라면 첫번째 term은 0이되고 두번째 term은 살아남습니다. <br>
따라서 이 공식은 -ln의 총합 즉, cross entropy를 정확히 표현하게 됩니다. 그래서 cross entropy는 언제 2개의 벡터(y와 p)가 유사하거나 다른지 표현하는 지표가 됩니다. 

![6](http://blogfiles.naver.net/MjAxNzEyMjhfMjQ0/MDAxNTE0NDI3ODk2MzM5.DXMrX2qgxekxqRqtow_F6SjmzNkuQsAEF6epHx_QgBcg.mlNzVaDestrFTRj2_EfkMRGop_0hRBnMVz9b0j0R1Ygg.PNG.infoefficien/20-2._Cross_Entropy_2.mp4_000320320.png?type=w1)

예를 들어 y = (1, 1, 0)이고 p = (0.8, 0.7, 0.1) 이면 CE = 0.69가 되어 상대적으로 낮은 CE를 갖게되어 발생 가능성이 높고 y = (0, 0, 1)이고 p = (0.8, 0.7, 0.1)이면 CE = 5.12가 되어 상대적으로 높은 CE를 갖게 되어 발생 가능성이 낮습니다.

    def cross_entropy(Y, P):
    	Y = np.float_(Y)
    	P = np.float_(P)
    	return np.sum(-Y*np.log(P) -(1-Y)*np.log(1-P))