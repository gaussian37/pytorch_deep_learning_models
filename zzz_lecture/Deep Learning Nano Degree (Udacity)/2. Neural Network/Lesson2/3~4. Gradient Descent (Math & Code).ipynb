{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent (Math & Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Gradient Descent Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-28T13:46:27.499068Z",
     "start_time": "2018-01-28T13:46:26.743444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkz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\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB/9k=\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/7sxA5Ap8AWM\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x2457df277f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"7sxA5Ap8AWM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](http://postfiles4.naver.net/MjAxODAxMjFfNjcg/MDAxNTE2NTI2NTczMjU5.XOMP2gmS8dV91chPOKiPimKHpH2iyKd0mHFdU41O37gg.sr8ZduXr9oInasbAJ4D_DWqg5tAAWohp3PfQtV_Crugg.PNG.infoefficien/Gradient_Descent-Math.mp4_000010677.png?type=w773)\n",
    "\n",
    "우리는 어떻게 Neural Network를 구성하여 output을 얻을 수 있는지 앞에서 배웠습니다. 그리고 구한 output을 통하여 모델의 예측값을 구할 수 있었습니다. 그러면 weight를 정확히 알기전에 어떻게 모델링을 할 수 있을까요?<br>\n",
    "먼저 weight가 정확할 것이라고 예상하는 값으로  세팅한 다음 점점 실제 model에 적합한 weight를 찾아갑니다.<br>\n",
    "\n",
    "![2](http://postfiles14.naver.net/MjAxODAxMjFfMjcz/MDAxNTE2NTI3NDE5OTcy.50k_JPEvTPfX280emHPd5DwoQezphqhiqmMMVPkr6IUg.tRljCyXA_LN_T_jxZm3bF2uCICnoD2jdfBvkV0LYTMYg.PNG.infoefficien/Gradient_Descent-Math.mp4_000088426.png?type=w773)\n",
    "\n",
    "첫번째, 우리의 model이 현재 weight를 사용하였을 때 얼마나 나쁜지 알 수 있어야 합니다. 현재 weight가 얼마나 나쁜지 알기 위해 이번 예제에서는 SSE를 사용해 보겠습니다.  weight의 좋고 나쁨을 확인하기 위한 척도인 loss function의 종류는 여러가지가 있습니다. 주로 학습에 빠른 CE(Cross Entropy)를 사용하지만 식이 매우 간단한 SSE는 좋은 예가 됩니다.<br>\n",
    "SSE 식을 조금 더 자세히 알아보겠습니다.\n",
    "\n",
    "![3](http://postfiles8.naver.net/MjAxODAxMjFfMjI3/MDAxNTE2NTI4NDM3MTUw.Eyyqk1unmkRBb1kJ7g9nGDqPBOmCgydlbILRinvm3DIg.B_ucDDLSCHhJ7gVH14yJu4i-686JcisdprpPw7dCWfcg.PNG.infoefficien/Gradient_Descent-Math.mp4_000033204.png?type=w773)\n",
    "\n",
    "먼저 정답 label y와 model의 output 값인 prediction $\\hat{y}$의 차이를 Error로 생각합니다. 하지만 이렇게 표현하면 Error의 크기가 양수가 될 수도 있고 음수도 될 수 있어 Error의 절대량을 표현하기가 어려워지므로 항상 양의 값이 되도록 표현하겠습니다.\n",
    "\n",
    "![4](http://postfiles8.naver.net/MjAxODAxMjFfMjU3/MDAxNTE2NTI4NjAwNjQ1.nWhoQCsMCDePjTOhNbQeRvOByZo0grPGy2HrIXNu0VUg.RKR179YZe2yrEOaNdIEuBk62RefVfgQr7gHl_chzO30g.PNG.infoefficien/Gradient_Descent-Math.mp4_000046104.png?type=w773)\n",
    "\n",
    "정답값과 예측값의 차이를 제곱하면 항상 양의 값을 구할 수 있습니다. 절대값을 사용하는 대신 제곱을 사용하는 이유는 좀 더 큰 에러에 더 많은 페널티를 주기 위해서 입니다. 또한 제곱 연산을 이용하는게 나중에 나올 수학 연산을 사용할 때 좀 더 계산이 간편합니다. \n",
    "\n",
    "![5](http://blogfiles.naver.net/MjAxODAxMjFfNzYg/MDAxNTE2NTI5MjY2MDYz.XVgn1Mj7p29VXRI1Qn4xKMNNURRzlHnjHbQVQ0AT9yUg.BHLlGov4a0OQ5G4n0voMP92RRq5IskESYLCoiJxSfdAg.PNG.infoefficien/Gradient_Descent-Math.mp4_000072450.png?type=w1)\n",
    "\n",
    "weight를 업데이트 하기 위해서는 전체 데이터 셋에 대하여 학습을 해야 합니다. 따라서 앞에서 만든 정답값과 예측값의 차이에 제곱한 값을 모든 데이터 셋 $\\mu$ 개에 대하여 모두 적용한 후 합을 구합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![6](http://blogfiles.naver.net/MjAxODAxMjFfMTk4/MDAxNTE2NTI5NDQ0MTMw.V8K_j0n9Jv5UW_6LLd6yyVKvlEbkrzf4QCr8UiX3ofQg.SrchIIaivRviEcVLK-yq6golkc1AJR1RWtI6I9GzL8Eg.PNG.infoefficien/Gradient_Descent-Math.mp4_000084421.png?type=w1)\n",
    "\n",
    "전체 합의 수식에 마지막으로 $\\frac{1}{2}$를 곱해줍니다. $\\frac{1}{2}$를 곱해주는 이유는 나중에 미분 연산 시 발생하는 상수값 2와 곱해주어 식을 간단하게 만들기 위해서 입니다. \n",
    "\n",
    "![7](http://blogfiles.naver.net/MjAxODAxMjFfNiAg/MDAxNTE2NTI5NTUyMTc3.uf_54lHz08hzluk5IkolKMGcwEIU4crLSo_7qGmIrD0g.tXrzSjip130crcnQSKb0D66QcsPu8CKORPj6AbH1tYIg.PNG.infoefficien/Gradient_Descent-Math.mp4_000088426.png?type=w1)\n",
    "\n",
    "다시 한번 정리하면 위의 형태가 SSE(Sum of the Squared Errors)가 됩니다.\n",
    "\n",
    "![8](http://blogfiles.naver.net/MjAxODAxMjFfMTM3/MDAxNTE2NTI5NTk2NDE5.Fk5rEXQtjfhdwryJOsGP5aJmSFiUSY3x9Wd5joFF5skg.Smw-Kee5Xb2vTXtF4_FmiGQzJXl4BUTj1V0Ia5SKW4Ug.PNG.infoefficien/Gradient_Descent-Math.mp4_000097178.png?type=w1)\n",
    "\n",
    "prediction값인 $\\hat{y}$을 풀어서 대입해 보면 위 식과 같습니다. $\\hat{y}$은 weight와 input의 linear combination의 형태로 연산이 되어있고 이 값은 최종적으로 activation function을 거치게 됩니다.\n",
    "\n",
    "![9](http://blogfiles.naver.net/MjAxODAxMjFfMTMx/MDAxNTE2NTMwMjg2NzY1.5nKjT6-zsUlg77YxqmWR6IKmJ_amDzJgMvdKRlhF-xgg.cVv-PY52q3RQQGn7PXp_KdgM68w-twPW-g0OhSQvhcAg.PNG.infoefficien/Gradient_Descent-Math.mp4_000129539.png?type=w1)\n",
    "\n",
    "위 식을 matrix 형태로 살펴 보면 input값 x와 target값인 y의 형태를 살펴 볼 수 있습니다. \n",
    "\n",
    "![10](http://blogfiles.naver.net/MjAxODAxMjFfMTE0/MDAxNTE2NTMwNDM3NjU2.h9SnpfY9t-59ZRwoCYVgF4VoA2UfHBOtBRTHamPnTqkg.9t_yahZdaSi48bPhcd8Vl97VtdS4_S40fqEJS5MOF8Ug.PNG.infoefficien/Gradient_Descent-Math.mp4_000155518.png?type=w1)\n",
    "\n",
    "SSE는 model의 성능이 얼마나 나쁜지 나타내므로 우리의 목표는 SSE 값을 가능한 한 최소 값으로 만드는 것입니다.  데이터 셋 전체를 고려하기 이전에 데이터 레코드 하나(데이터 행 한개)만 고려하여 좀 더 단순한 형태를 살펴보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![11](http://blogfiles.naver.net/MjAxODAxMjFfNTQg/MDAxNTE2NTMwNjkxMjk1.ouJDgbah4ssXyGpHxYRZoU6mXYCfWNTSxGv4h3-mZXcg.b1lpypL8L-EwwbZ4xGCYQP4vEHKPvnALcLzvO_IpFZ0g.PNG.infoefficien/Gradient_Descent-Math.mp4_000158603.png?type=w1)\n",
    "\n",
    "데이터 레코드 하나만 고려 한다면 SSE의 $\\mu$ = 1이므로 $\\Sigma$ 를 없애도 됩니다. 따라서 위와 같이 식을 간단히 적을 수 있습니다. \n",
    "\n",
    "![12](http://blogfiles.naver.net/MjAxODAxMjFfMzAw/MDAxNTE2NTMwOTg4MTc2.a10j43WXFCpb5E3sPvBChUSyS70ZtSvmIFG8L56lk5gg.KdGGf21gy7RKjJ6BjaMyl3JUA2Du7M3Q49M1zxzQZyYg.PNG.infoefficien/Gradient_Descent-Math.mp4_000169197.png?type=w1)\n",
    "\n",
    "다시 $\\hat{y}$을 자세히 표현한 형태로 풀어서 적어보겠습니다.\n",
    "\n",
    "![13](http://blogfiles.naver.net/MjAxODAxMjFfOTMg/MDAxNTE2NTMxMDkxNTQ0.QVWU05dmd4aM3az0aFlzeypO7XYShBCJrO81bmfzaGkg.kOLHPqOenzWXraNjtig2hVLzdiX0TFoiUZb-3IfKzp8g.PNG.infoefficien/Gradient_Descent-Math.mp4_000207111.png?type=w1)\n",
    "\n",
    "위에서 전개한 식을 살펴보면 변수는 weight인 w가 됩니다. w 값의 변화에 따라 SSE 값이 변하게 되고 앞에서 말했듯이 모델의 성능을 개선하기 위하여 SSE를 가장 최소화 하도록 해야합니다. 위 Error 식은 2차원 형태로 되어 있고 변수는 w이므로  w에 따른 2차원 함수 그래프로 나타낼 수 있습니다. 이 때 E의 값을 최소로 만드는 w를 찾는 것이 학습의 목적이 될 수 있습니다. 이 때 한번에 E를 최소로 만드는 w를 찾는 것이 어렵기 때문에 gradient descent 방법을 이용하여 점진적으로 찾아 나아가야 합니다. Error를 줄이기 위해서는 $\\Delta w = -gradient $ 방향으로 weight 값을 변경해야 합니다. \n",
    "\n",
    "![14](http://blogfiles.naver.net/MjAxODAxMjFfMjA4/MDAxNTE2NTMyMDIyMTk5.Og34qTS4R-D2XOKvIOmf-XWimGRMbV-7xFD1Pgk-hgYg.LocwoDBn-HMKBcUogeyKQ5NzFkBH1JLbez3-y70cdRkg.PNG.infoefficien/Gradient_Descent-Math.mp4_000253953.png?type=w1)\n",
    "\n",
    "weight를 업데이트 하기 위해서는 $w_{i}$에 $\\Delta w_{i}$를 더해줍니다. 이 때 $\\Delta w_{i}$는 $\\frac{-\\partial E}{\\partial w_{i}}$값에 비례합니다.  $\\frac{-\\partial E}{\\partial w_{i}}$ 값에 Learning rate $\\eta$를 곱한 값이 최종적으로 업데이트할 값인 $\\Delta w_{i}$ 가 됩니다. 이 때 Learning rate는 Gradient를 얼마나 반영할 지에 대한 hyper parameter가 됩니다. \n",
    "\n",
    "![15](http://blogfiles.naver.net/MjAxODAxMjFfMTg3/MDAxNTE2NTMyNzc2MDM0.KcXnmxYFhcDbudhQ325u9U7Ha6XVNe-9khGiYmdg590g.KBh5hXTXEoPNV9bXs-YSxKgNJyZEcJRFMC39cRrvPUkg.PNG.infoefficien/Gradient_Descent-Math.mp4_000272993.png?type=w1)\n",
    "\n",
    "따라서 $w_{i}$에 대하여 E를 미분하면 위와 같이 전개할 수 있습니다. 이 연산을 좀 더 쉽게 하려면 Chain rule을 적용해야 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![16](http://blogfiles.naver.net/MjAxODAxMjFfMzUg/MDAxNTE2NTMzNDY5NjI2.q9AWoekGLEopzkR56Tc1CTTI4T-sqUpLIx2Oe4dw8T8g.51WLENCirjNEDxS90-xDMjBw4isUq_0o3fkoT6LB-JAg.PNG.infoefficien/Gradient_Descent-Math.mp4_000323289.png?type=w1)\n",
    "\n",
    "만약 변수 z에 대하여 함수 p의 미분을 구하고 싶으면 함수 p는 또다른 함수 q에 의존적이기 때문에 Chain rule을 적용하면 함수 p는 q에 의하여 미분이 되고 다시 함수 q는 변수 z에 대하여 미분이 됩니다. <br>\n",
    "이 형태를 SSE에 접목시켜 보면 $ q = (y - \\hat{y}(w_{i}))$ 이고 $ p = \\frac{1}{2}q(w_{i})^{2}$으로 표현할 수 있으므로 $E = p(q(w_{i}))$가 됩니다. 따라서 위 식과 같이 Chain rule을 적용하여 전개할 수 있습니다. \n",
    "\n",
    "![17](http://blogfiles.naver.net/MjAxODAxMjFfMTYg/MDAxNTE2NTM0ODczNDU3.0f6zEZHD6vk_wrTd9tgSqUvm46EJy49DlyMg4eYii1wg.RHBQobRpDuyW-DYaGJJZcmsp7pQw7DQ5AdouJHdgkYYg.PNG.infoefficien/image_7684643951516534610033.png?type=w1)\n",
    "\n",
    "정답값인 y는 미분식과 전혀 상관이 없고 $ \\hat{y} $ 내의 변수 weight만 관련이 있음을 참조하면 위 식과 같이 편미분 식을 전개할 수 있습니다.\n",
    "\n",
    "![18](http://blogfiles.naver.net/MjAxODAxMjFfMjI5/MDAxNTE2NTM1MjQ1NTQw.Lnw2lxNbRRseclJtwCFtj3BGn_abUODFOZyCsIhH_2og.znyYWYK8epMRsjDt1xAbApYkzl33OlwpaN9_5OcBA6Ug.PNG.infoefficien/Gradient_Descent-Math.mp4_000357452.png?type=w1)\n",
    "\n",
    "$ \\hat{y} $은 weight와 input x의 linear combination에 activation을 적용한 형태이므로 chain rule을 계속 적용하면 위와 같이 식을 전개할 수 있습니다.\n",
    "\n",
    "![19](http://blogfiles.naver.net/MjAxODAxMjFfMTEx/MDAxNTE2NTM1NTEzMDUz.9QpaFKHpgTOxPQ-HQMCOCo3Q44qcbh4BbZXs0OOeHTsg.qbgm-Kd6dWoLQ9xPWeNNQAr9xXUaWTZw7UDKBYmNJw0g.PNG.infoefficien/Gradient_Descent-Math.mp4_000381265.png?type=w1)\n",
    "\n",
    "가장 안쪽의 식을 $ w_{i} $에 대하여 편미분을 하면 최종적으로 $ w_{i} $와 곱으로 연결된 $ x_{i} $만 남고 나머지는 편미분 결과 0이 됩니다. \n",
    "\n",
    "![20](http://blogfiles.naver.net/MjAxODAxMjFfMjQy/MDAxNTE2NTM1NjgzNTIz.Y7V1rYch39pbe_DOKp0PXpcuiWDQhsbbfXsJUmpMu_Yg.mRTKGU0kYlz6JhC_9PYYVRzOZMu6r8_1JJi8w6L1WS4g.PNG.infoefficien/Gradient_Descent-Math.mp4_000398014.png?type=w1)\n",
    "\n",
    "따라서 마지막 식에서 $x_{i}$만 남게되어 $\\Delta w_{i} $의 값을 learning rate와 chain rule의 결과 값을 곱한 것으로 표현할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![21](http://blogfiles.naver.net/MjAxODAxMjFfMjQ5/MDAxNTE2NTM1ODk4MTcw.VPc-h8Sn2tiFNJBpIFg1p6J_grVyoUnZRFWe0fw-hc0g.29lEtq3GwdJtUVKlNhGPKta7chXM-5YpVrGLuYna77og.PNG.infoefficien/Gradient_Descent-Math.mp4_000416395.png?type=w1)\n",
    "\n",
    "따라서 update 시 더해야 하는 term을 $\\delta$ 로 표현하면 $\\Delta w_{i} = \\eta*\\delta*x_{i}$로 정의할 수 있습니다.\n",
    "\n",
    "![22](http://blogfiles.naver.net/MjAxODAxMjFfNTkg/MDAxNTE2NTM2MDMwODkx.dV_bUYiudqNWyC1RrhVBDh_UaSSOid8Q9FJuvpZl9Lcg.VSxiDllRUuf3fK1uH-Og_1e98kYp-YCZJAF6XvtezGwg.PNG.infoefficien/Gradient_Descent-Math.mp4_000435849.png?type=w1)\n",
    "\n",
    "이 때까지 하나의 output만 고려하여 gradient를 구했다면 이제 여러개의 output에 대하여 응용해보면 $w_{ij}$ 에 대하여 모두 연산을 적용하면 됩니다. 위의 예에서는 output의 갯수가 2개아므로 j = 1, 2 값을 가지게 됩니다.\n",
    "\n",
    "![23](http://blogfiles.naver.net/MjAxODAxMjFfMTE4/MDAxNTE2NTM2Mzk3NTQ0.WKq55VFH4GZ-vyrxjt7Wnhj_g3LHGdBGAgPBGVllA0wg.9KHhNh0OphPaEHBxt9fLru_Ys8qLPRnY2eBnxasS6Bsg.PNG.infoefficien/Gradient_Descent-Math.mp4_000455389.png?type=w1)\n",
    "\n",
    "따라서 복수개의 output을 가지려면 복수개(j) 만큼의 ERROR TERM을 가집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Gradient Descent Code\n",
    "\n",
    "weight를 업데이트 하기 위해서는 아래 식을 이용해야 합니다.\n",
    "\n",
    "$$ \\Delta w_{i} = \\eta \\delta x_{i} $$\n",
    "\n",
    "$$ \\eta = (y - \\hat{y}) f'(h) = (y - \\hat{y}) f'(\\sum w_{i}x_{i}) $$\n",
    "\n",
    "$$ output error = y - \\hat{y} $$\n",
    "\n",
    "$$ f'(h) = derivative of activation function $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T22:07:35.236538Z",
     "start_time": "2018-01-21T22:07:35.205956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Input data\n",
    "x = np.array([0.1, 0.3])\n",
    "# Target\n",
    "y = 0.2\n",
    "# Input to output weights\n",
    "weights = np.array([-0.8, 0.5])\n",
    "\n",
    "# The learning rate, eta in the weight step equation\n",
    "learnrate = 0.5\n",
    "\n",
    "# the linear combination performed by the node (h in f(h) and f'(h))\n",
    "h = x[0]*weights[0] + x[1]*weights[1]\n",
    "# or h = np.dot(x, weights)\n",
    "\n",
    "# The neural network output (y-hat)\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# output error (y - y-hat)\n",
    "error = y - nn_output\n",
    "\n",
    "# output gradient (f'(h))\n",
    "output_grad = sigmoid_prime(h)\n",
    "\n",
    "# error term (lowercase delta)\n",
    "error_term = error * output_grad\n",
    "\n",
    "# Gradient descent step \n",
    "del_w = [ learnrate * error_term * x[0],\n",
    "          learnrate * error_term * x[1]]\n",
    "# or del_w = learnrate * error_term * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T22:08:17.728963Z",
     "start_time": "2018-01-21T22:08:17.663289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.6899744811276125\n",
      "Amount of Error:\n",
      "-0.1899744811276125\n",
      "Change in Weights:\n",
      "[-0.02031869 -0.04063738 -0.06095608 -0.08127477]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    # Derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
    "\n",
    "### Calculate one gradient descent step for each weight\n",
    "### Note: Some steps have been consilated, so there are\n",
    "###       fewer variable names than in the above sample code\n",
    "\n",
    "# TODO: Calculate the node's linear combination of inputs and weights\n",
    "h = np.dot(x, w)\n",
    "\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# TODO: Calculate the error term\n",
    "#       Remember, this requires the output gradient, which we haven't\n",
    "#       specifically added a variable for.\n",
    "error_term = error * sigmoid_prime(h)\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
