{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "작성일 : 2018-01-13 16:00:32 <br>\n",
    "작성자 : Gauss Kim <br>\n",
    "참조 : https://youtu.be/29PmNG7fuuM <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network에서 우리의 목적은 <span class=\"mark\">신경망에 대한 가중치</span>를 찾는 것 입니다.<br>\n",
    "네트워크는 가능한 한 실제 값에 가까운 예측을 해야합니다. 이것을 측정하기 위해 우리는 예측이 얼마나 틀린지에 대한 척도 인 Error를 사용합니다. 여기서 사용할 Function은 Sum of Squred Error(SSE)입니다.\n",
    "\n",
    "$E = \\frac{1}{2}\\sum_{\\mu}\\sum_{j} [y^{\\mu}_j - \\hat{y}^{\\mu}_j]^2$\n",
    "\n",
    "이 식에서 $\\hat{y}$은 prediction이고 y는 정답 라벨 입니다. 이 식을 통하여 모든 출력 단위 j에 대한 합계를 취하고 모든 데이터 포인트 μ에 대한 또 다른 합계를 취하여 총합을 하게 됩니다. \n",
    "먼저 <span class=\"mark\">j는 네트워크의 output 단위</span>를 나타냅니다. 따라서 이 합계는 각각의 출력 단위에 대해 네트워크에서 참값 y와 예측 값 $\\hat{y}$의 차이를 찾습니다. y와 $\\hat{y}$ 차이를 제곱 한 다음 모든 제곱을 합산합니다.\n",
    "\n",
    "두번째로 <span class=\"mark\">μ에 대한 다른 합계는 모든 데이터 포인트에 대한 합계</span>입니다. 따라서 각 데이터 포인트에 대해 각 출력 단위에 대한 제곱 차의 내부 합을 계산합니다. 그리고 각각의 μ에 대하여 Sum of Squred Error를 계산한 값들을 모두 더합니다.\n",
    "\n",
    "**SSE를 사용하여 Error를 계산할 때 장점**이 있습니다.<br>\n",
    "① 모든 y와 y^의 차이를 양수로 만듭니다.<br>\n",
    "② 큰 값에 대하여 더 큰 페널티를 주어 y와 y^의 차이가 큰 경우 결과 값을 더 크게 만들어 버립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴런 네트워크의 결과 인 prediction은 weight에 달려 있음을 기억하면, 먼저 Prediction의 수식 형태는  $$ \\hat{y}^{\\mu}_{j} = f(\\sum_{i} w_{ij} x^{\\mu}_{i}) $$ 이고 Error Function의 값 또한 weight에 영향을 받는 $$ E = \\frac{1}{2} \\sum_{\\mu}\\sum_{j}[y^{\\mu}_{j} - f(\\sum_{i} w_{ij} x^{\\mu}_{i})]^2 $$ 가 됨을 배웠습니다.\n",
    "\n",
    "우리의 목적은 Error 값을 가능한한 작게 만드는 것이고 weight를 변경하여 Error 값을 조정할 수 있습니다. 따라서 E<span class=\"mark\">rror를 최소한으로 줄여줄 수 있는 최적의 weight를 찾아야하고 최적의 weight를 찾기 위하여 gradient descent 방법을 사용해야 합니다.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](http://postfiles15.naver.net/MjAxODAxMTNfNjkg/MDAxNTE1ODIwNzcxNjI4.vK01v_1Rgb4dliOzsIpkhs25zpz9GWAz8P4oVGz35Vsg.V3M6ZLOaJMCT4-5Le5xJJkNbGgFGgEmwQJ_ucW3i2FEg.PNG.infoefficien/1._Gradient_Descent.mp4_000022866.png?type=w773)\n",
    "\n",
    "Gradient Descent를 적용하기 위해 다시 Error를 산에 비유해 보겠습니다. 우리가 산 정상에 있을 때 어떻게 바닥 까지 내려올까요? 여러가지 길이 있을 것입니다.<br>\n",
    "위와 같이 각 지점에서 최대한 내려갈 수 있는(Error 값을 최대한 줄일 수 있는) 방향으로 최대한 내려온 뒤 바닥에 닿을 때 까지(Error가 0에 수렴할 떄 까지) 이 방법을 계속 반복하면 결국 바닥에 닿게 됩니다.(Error가 0에 수렴하게 됩니다.)\n",
    "\n",
    "![2](http://postfiles6.naver.net/MjAxODAxMTRfOSAg/MDAxNTE1OTIyOTI5MTg5.yB3Zq4-gHcSoLStR74hJWHojcqCDSfu0mbdeeukO9rQg.5SNPrnawBuk4h1G6uMAKYPHoxyqzT55Viz1_21260mUg.GIF.infoefficien/Errorcost.gif?type=w773)\n",
    "\n",
    "![3](http://postfiles7.naver.net/MjAxODAxMTNfOTIg/MDAxNTE1ODIxMjA3MDUx.LpjrwAu0mNrCXIEiUvLxgdLzv9lCO4JUlCxB0KUsbmwg.vODkGysGSVoqt_rKPmF2581PRO6y6ZOqS398hMdM4Ocg.PNG.infoefficien/1._Gradient_Descent.mp4_000061853.png?type=w773)\n",
    "\n",
    "위와 같은 방법으로 사용되는 Gradient Descent는 딥러닝 뿐만 아니라 머신러팅 전반에 걸쳐서 사용되고 있고 산 = problem, 땅 = solution, 정상→지면 의 경로 찾기 = gradient descent algorithm이 됩니다.\n",
    "\n",
    "gradient descent를 이용하면 우리의 목표(Error를 0에 수렴)를 위해 여러번의 small step으로 목표점 까지 도달할 수 있습니다. 이 경우 Error를 줄이는 방향으로 weight를 변경할 계획입니다. 비유를 계속 하면 Error는 산이고 Bottom은 목표 입니다. <br>\n",
    "산을 내려가는 가장 빠른 방법은 가장 가파른 기울기로 내려가는 방법이기 때문에 가장 기울기가 심한 방법으로 산을 내려 가겠습니다.  <br>\n",
    "우리는 SSE 와 Gradient(기울기, 변화율)를 이용하여 방향을 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변화율을 계산하기 위해 미분을 사용해야 합니다.<br>\n",
    "f(x)의 x에 대한 미분은 f'(x) 즉, 함수 f(x)의 x점에서의 변화율을 구합니다. <br>\n",
    "예를 들어 f(x) = x2 이라고 하면 f'(x) = 2x가 됩니다. 만약 x = 2일 때 f'(2) = 4가 됩니다.<br>\n",
    "\n",
    "![4](http://postfiles1.naver.net/MjAxODAxMTNfMTcg/MDAxNTE1ODIzMzY5NDA0.HcoXkyJcjldJrDrXNCh4Zb-9DSqz3YY6HyYY027eB1kg.fNDn8_dJGn1sHXrskymZnLldMEpquwwQF1RzM1x5Imgg.PNG.infoefficien/derivative-example.png?type=w773)\n",
    "\n",
    "gradient는 둘 이상의 변수로 구성된 함수를 미분하여 표시한 것입니다. 우리는 미분을 사용하여 각각의 weight에 따라 변하는 Error Function의 gradient를 찾을 수 있습니다. \n",
    "\n",
    "![5](http://blogfiles.naver.net/MjAxODAxMTNfOCAg/MDAxNTE1ODI0ODU5NDMy.sA51ZrKGTtXvuORcvPVPhnKNuOvJuIkkgr6WGDsN7QYg.06S1zuAALklwi-U8db6vAyXDBSmz5i8BiZZP_unisK0g.PNG.infoefficien/gradient-descent.png?type=w1)\n",
    "\n",
    "위는 2 개의 weight w1, w2의 변화에 따라서 달라지는 Error를 나타냅니다. <br> \n",
    "같은 등고선의 점이 같은 오차를 갖고 더 어두운 외곽 윤곽선이 더 큰 오차를 가집니다. <br>\n",
    "각 단계에서 Error 및 Gradient를 계산 한 다음 이를 사용하여 각 weight를 얼마만큼 변경해야하는지 결정 해야 합니다. <br>\n",
    "이 프로세스를 반복하면 결국 중간에있는 검은 점인 Error Function의 최소값에 가까운 weight를 찾습니다.\n",
    "\n",
    "![6](http://blogfiles.naver.net/MjAxODAxMTNfMjMw/MDAxNTE1ODI1NDM2OTg3.qThUNn_OPehtrzELIyMJu-JgBS-Zzr-nqkMY2-CGpWEg.nq3NweJw8b7VS8YiOld-NS98JhBphEzz_H7uvuTYlmEg.PNG.infoefficien/local-minima.png?type=w1)\n",
    "\n",
    "weight는 gradient의 값에 따른 위치로 이동하기 때문에  Error가 낮은  지점으로 이동하지만 가장 낮은 지점으로 이동하지는 않습니다.\n",
    "이렇게 가장 낮은 지점은 아니지만 Gradient 에 의해 이동한 Error가 낮은 지점을 local minima 라고 합니다. weight가 초기에 잘못 설정되면 gradient descent는 Error가 가장 낮은 지점이 아닌 local minima 방향으로 Error를 줄일 수 있습니다.\n",
    "\n",
    "[local minima를 피하기 위한 방법](http://ruder.io/optimizing-gradient-descent/index.html#momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
