{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T17:41:27.189814Z",
     "start_time": "2018-05-05T17:41:27.175775Z"
    }
   },
   "source": [
    "![1](nb_images/2.1.png)\n",
    "\n",
    "앞의 강의에서 설명한 바와 같이 학습자, 의사 결정을 하는 대상을 Agent 라고 하였습니다. 그러면 흔히 RL이 학습을 하는 프로세스가 어떻게 되는지 알아보도록 하겠습니다. <br>\n",
    "다음에 설명한 모든 프로세스는 RL을 할 때 모든 Agent가 겪게 되는 순서 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T17:41:37.276649Z",
     "start_time": "2018-05-05T17:41:37.263614Z"
    }
   },
   "source": [
    "![2](nb_images/2.2.png)\n",
    "\n",
    "먼저 RL에서는 Agent와 Environment가 존재합니다. 첫 timestep에서는 Environment → Agent로 Observation 을 전달 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3](nb_images/2.3.png)\n",
    "\n",
    "이 때 Agent는 특정 Action을 정하여 Environment로 전달합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4](nb_images/2.4.png)\n",
    "\n",
    "그 다음 timestep 부터는 Environment는 observation 값과 동시에 reward 를 Agent에 전달합니다. 이 reward는 이전 timestep에서 Agent가 Action에 따른 결과 입니다.\n",
    "새로운 Environment의 observation과 reward를 받으면 Agent는 새로운 action을 취하게 됩니다. 이 작업을 반복하면서 RL 을 하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![5](nb_images/2.5.png)\n",
    "\n",
    "여기서 Observation은 State라고도 표현하기도 합니다. 그러면 RL 프로세스를 좀 더 자세하게 표현해 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![6](nb_images/2.6.png)\n",
    "\n",
    "앞에서 설명한 바와 같이 첫 step에서는 Environment → Agent로 $S_{0}$ 만 전달하고 $A_{0}$를 전달 받습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![7](nb_images/2.7.png)\n",
    "\n",
    "이제 그 다음 스텝인 timestep 1에서 부터는 Environment → Agent로 $R_{1}$과 $S_{1}$을 같이 전달해주고 그 결과로 $A_{1}$을 전달 받습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![8](nb_images/2.8.png)\n",
    "\n",
    "Environment와 Agent의 상호 작용 순서를 보면 위와 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![9](nb_images/2.9.png)\n",
    "\n",
    "이 때 RL에서 핵심이 되는 R = Reward에 집중할 필요가 있습니다. Agent의 목적은 Reward를 어떻게 하면 최대화 할것인지 선택하는 것이기 때문입니다. 그리고 중요한 것은 \"<span class=\"mark\">Cumulative</span>\" Reward 입니다. 눈에 보이는 reward 뿐만 아니라 장기적으로 어떻게 Reward를 최대화 할것인지가 중요합니다. \n",
    "\n",
    "Agent는 Environment와의 상호 작용을 통하여 전체 Rule을 결정해야 합니다. 이 때 Agent가 최적화 하는 Rule은 Environment가 제공하는 Rule이고 Agent를 그 Rule을 학습을 한다고 생각하면 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
