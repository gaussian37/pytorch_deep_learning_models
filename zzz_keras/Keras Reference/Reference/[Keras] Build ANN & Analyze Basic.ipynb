{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span class=\"mark\">Preprocessing data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-24T16:21:07.225775Z",
     "start_time": "2018-02-24T16:21:04.928586Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # numpy\n",
    "from random import randint # data set을 만들기 위한 random library\n",
    "from sklearn.preprocessing import MinMaxScaler # data normaliza를 위한 scale 라이브러리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 예제\n",
    "- 13 ~ 65세를 위한 어떤 약을 만들기 위해 임상실험 진행\n",
    "- 2000명의 표본으로 반은 65세 이하를 대상으로 나머지 반은 65세 이상으로 진행\n",
    "- 95%의 확률로 65세 이상은 부작용 발생\n",
    "- 95%의 확률로 65세 미만은 부작용 미발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-24T16:21:07.238782Z",
     "start_time": "2018-02-24T16:21:07.229757Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_data : train 데이터, Y_data : label\n",
    "# 13 ~ 64세는 95%는 부작용 미발생, 5% 발생\n",
    "# 65세 이상(100세 이하)는 95%는 부작용 발생, 5%는 미발생\n",
    "# 2000개의 데이터 = 950(13 ~64 : 0) + 50(13 ~ 64 : 1) + 950(65 ~ 100 : 1) + 50(65 ~ 100 : 0)\n",
    "\n",
    "# Train data 생성, 나이는 13 ~ 64세로 랜덤하게 만들며 1000개의 샘플을 만듭니다.\n",
    "X_data = np.random.randint(low = 13, high = 64, size = 1000) \n",
    "# Test data 생성, 나이는 13 ~ 64세이고 95%는 무작용 미발생, 5%는 발생하도록 만듭니다.\n",
    "Y_data = np.zeros(950) # 13 ~ 64세 95%는 부작용 미발생\n",
    "Y_data = np.append(Y_data, np.ones(50)) # 13 ~ 64 95%는 부작용 발생\n",
    "\n",
    "# Training data 생성, 나이는 65~100세로 랜덤하게 만들며 1000개의 샘플을 만듭니다.\n",
    "X_data = np.append(X_data, np.random.randint(low = 65, high = 100, size = 1000))\n",
    "# test data 생성, 나이는 65 ~ 100세이고, 95%는 부작용 발생, 5%는 부작용 미발생하도록 만듭니다.\n",
    "Y_data = np.append(Y_data, np.ones(950)) # 65 ~ 100세 95%는 부작용 발생\n",
    "Y_data = np.append(Y_data, np.zeros(50)) # 65 ~ 100세 5%는 부작용 미발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-24T16:21:07.259866Z",
     "start_time": "2018-02-24T16:21:07.242792Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\infoe\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:420: DataConversionWarning: Data with input dtype int32 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Input data scale : 너무 큰 input 값은 weight가 수렴하는 데 오래 걸려\n",
    "# 학습하는 데 시간이 오래 걸리므로 data scale을 통하여 값을 줄여주어 빠르게 학습하도록 합니다.\n",
    "\n",
    "# [0, 1] 사이의 값으로 스케일 변화를 위한 변수 생성\n",
    "scalar = MinMaxScaler(feature_range= (0, 1)) \n",
    "# (None, 1) shape으로 [0,1] 범위로 스케일 변경\n",
    "scaled_X_data = scalar.fit_transform(X_data.reshape(-1, 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-24T16:21:07.316990Z",
     "start_time": "2018-02-24T16:21:07.312980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_data :  [19 52 28 ... 89 78 69]\n",
      "X_data.shape :  (2000,)\n",
      "scaled_X_data :  [[0.06976744]\n",
      " [0.45348837]\n",
      " [0.1744186 ]\n",
      " ...\n",
      " [0.88372093]\n",
      " [0.75581395]\n",
      " [0.65116279]]\n",
      "(2000, 1)\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_data : \", X_data) # Train data\n",
    "print(\"X_data.shape : \", X_data.shape) # Train data shape\n",
    "print(\"scaled_X_data : \", scaled_X_data) # scaled train data\n",
    "print(scaled_X_data.shape) # scaled train data shape\n",
    "print(Y_data) # Label data\n",
    "print(Y_data.shape) # label data shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span class=\"mark\">Neural Network</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-24T16:21:15.877809Z",
     "start_time": "2018-02-24T16:21:09.524864Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\infoe\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "from keras import backend as K \n",
    "from keras.models import Sequential # Sequential : model 선언\n",
    "from keras.layers import Activation # Activation 사용\n",
    "from keras.layers.core import Dense # Dense layer 사용\n",
    "from keras.optimizers import Adam # Adam optimizer 사용\n",
    "from keras.metrics import categorical_crossentropy # cross entropy 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sequential을 선언한 후 layer를 쌓습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-24T16:21:25.329038Z",
     "start_time": "2018-02-24T16:21:25.240825Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sequential()을 이용하여 model을 선언합니다.\n",
    "model = Sequential()\n",
    "# model의 전체 틀은 model.summary()를 이용하여 확인하면 보기 편합니다.\n",
    "\n",
    "# Dense layer를 추가합니다. 1-dim vector → (None, 16)\n",
    "# activation으로 ReLU를 사용합니다.\n",
    "model.add(Dense(16, input_shape=(1,), activation = 'relu'))\n",
    "# Dense layer를 추가합니다. → (None, 32)\n",
    "# activation으로 ReLU를 사용합니다.\n",
    "model.add(Dense(32, activation='relu'))\n",
    "# Dense layer를 추가합니다. → (None, 2)\n",
    "# 출력층이므로 activation으로 softmax를 사용합니다.\n",
    "model.add(Dense(2, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 쌓은 layer의 정보를 얻습니다. shape 및 parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T22:01:19.896406Z",
     "start_time": "2018-02-11T22:01:19.893398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 642\n",
      "Trainable params: 642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 각 layer의 output shape과 Parameter의 수를 요약해서 볼 수 있습니다.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T22:01:20.483927Z",
     "start_time": "2018-02-11T22:01:20.445826Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.compile을 이용하여 optimizer와 loss를 선언합니다. \n",
    "# optimizer(학습 방법) : Adam \n",
    "# loss function(loss 계산 방법) : sparse_categorical_cross_entropy (one-hot 아닐 때는 sparse 써야한다.)\n",
    "# metrics(performance 판단 기준) : accuracy\n",
    "model.compile(optimizer = Adam(lr = 0.01),\n",
    "              loss = 'sparse_categorical_crossentropy', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T22:01:27.852493Z",
     "start_time": "2018-02-11T22:01:20.694699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/20\n",
      "1600/1600 [==============================] - 1s 345us/step - loss: 0.3313 - acc: 0.9075 - val_loss: 0.5730 - val_acc: 0.8750\n",
      "Epoch 2/20\n",
      "1600/1600 [==============================] - 0s 206us/step - loss: 0.1752 - acc: 0.9500 - val_loss: 1.0194 - val_acc: 0.8750\n",
      "Epoch 3/20\n",
      "1600/1600 [==============================] - 0s 202us/step - loss: 0.1556 - acc: 0.9581 - val_loss: 1.2400 - val_acc: 0.8750\n",
      "Epoch 4/20\n",
      "1600/1600 [==============================] - 0s 214us/step - loss: 0.1579 - acc: 0.9506 - val_loss: 1.2253 - val_acc: 0.8425\n",
      "Epoch 5/20\n",
      "1600/1600 [==============================] - 0s 196us/step - loss: 0.1599 - acc: 0.9519 - val_loss: 1.3554 - val_acc: 0.8750\n",
      "Epoch 6/20\n",
      "1600/1600 [==============================] - 0s 212us/step - loss: 0.1502 - acc: 0.9550 - val_loss: 1.4979 - val_acc: 0.8750\n",
      "Epoch 7/20\n",
      "1600/1600 [==============================] - 0s 207us/step - loss: 0.1498 - acc: 0.9600 - val_loss: 1.6314 - val_acc: 0.8750\n",
      "Epoch 8/20\n",
      "1600/1600 [==============================] - 0s 214us/step - loss: 0.1438 - acc: 0.9581 - val_loss: 1.6958 - val_acc: 0.8750\n",
      "Epoch 9/20\n",
      "1600/1600 [==============================] - 0s 215us/step - loss: 0.1362 - acc: 0.9613 - val_loss: 1.7060 - val_acc: 0.8750\n",
      "Epoch 10/20\n",
      "1600/1600 [==============================] - 0s 209us/step - loss: 0.1360 - acc: 0.9637 - val_loss: 1.6460 - val_acc: 0.8425\n",
      "Epoch 11/20\n",
      "1600/1600 [==============================] - 0s 197us/step - loss: 0.1586 - acc: 0.9531 - val_loss: 1.6707 - val_acc: 0.8750\n",
      "Epoch 12/20\n",
      "1600/1600 [==============================] - 0s 216us/step - loss: 0.1518 - acc: 0.9569 - val_loss: 1.7323 - val_acc: 0.8750\n",
      "Epoch 13/20\n",
      "1600/1600 [==============================] - 0s 217us/step - loss: 0.1398 - acc: 0.9606 - val_loss: 1.8157 - val_acc: 0.8750\n",
      "Epoch 14/20\n",
      "1600/1600 [==============================] - 0s 212us/step - loss: 0.1479 - acc: 0.9581 - val_loss: 1.7453 - val_acc: 0.8750\n",
      "Epoch 15/20\n",
      "1600/1600 [==============================] - 0s 197us/step - loss: 0.1413 - acc: 0.9587 - val_loss: 1.7323 - val_acc: 0.8750\n",
      "Epoch 16/20\n",
      "1600/1600 [==============================] - 0s 215us/step - loss: 0.1423 - acc: 0.9594 - val_loss: 1.7158 - val_acc: 0.8425\n",
      "Epoch 17/20\n",
      "1600/1600 [==============================] - 0s 204us/step - loss: 0.1389 - acc: 0.9637 - val_loss: 1.7353 - val_acc: 0.8575\n",
      "Epoch 18/20\n",
      "1600/1600 [==============================] - 0s 224us/step - loss: 0.1339 - acc: 0.9650 - val_loss: 1.8001 - val_acc: 0.8750\n",
      "Epoch 19/20\n",
      "1600/1600 [==============================] - 0s 222us/step - loss: 0.1300 - acc: 0.9681 - val_loss: 1.8932 - val_acc: 0.8750\n",
      "Epoch 20/20\n",
      "1600/1600 [==============================] - 0s 217us/step - loss: 0.1410 - acc: 0.9625 - val_loss: 1.8132 - val_acc: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x151601e9c18>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit을 통하여 학습을 합니다.\n",
    "# input data : scaled_X_data \n",
    "# output data : Y_data\n",
    "# validation ratio : 0.2\n",
    "# batch size : 10\n",
    "# epochs : 20\n",
    "# shuffle (epoch 마다 batch 순서가 shuffle 되는 지 유무) : True (default : True)\n",
    "\n",
    "model.fit(scaled_X_data, Y_data, validation_split = 0.2, batch_size=16, epochs=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T22:15:57.365725Z",
     "start_time": "2018-02-11T22:15:57.293993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.4078732e-01 5.9212748e-02]\n",
      " [9.4078749e-01 5.9212506e-02]\n",
      " [9.0622270e-01 9.3777336e-02]\n",
      " ...\n",
      " [1.6088597e-21 1.0000000e+00]\n",
      " [1.5854526e-07 9.9999988e-01]\n",
      " [3.4541810e-16 1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# model.predict를 이용하여 input에 대한 결과를 예측합니다.\n",
    "# input shape과 동일하게 맞추어서 데이터를 입력하면 됩니다.\n",
    "# 예제에서는 학습하였던 데이터를 재사용해서 입력해 보겠습니다.\n",
    "\n",
    "# model.predict를 사용하면 각 클래스의 확률을 출력합니다.\n",
    "predictions = model.predict(scaled_X_data, batch_size=16)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T22:15:39.848378Z",
     "start_time": "2018-02-11T22:15:39.766390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# model.predict_classes를 사용하면 각 클래스 확률을 비교하여 가장 큰 확률을 클래스를 출력합니다.\n",
    "# 위의 model.predict의 첫 3개는 0 클래스의 확률이 높고 끝 3개는 1 클래스 확률이 높습니다.\n",
    "# 따라서 아래 결과의 첫 3개는 0 클래스이고 끝 3개는 1 클래스입니다.\n",
    "class_predictions = model.predict_classes(scaled_X_data, batch_size=16)\n",
    "print(class_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 277.181818,
   "position": {
    "height": "40px",
    "left": "1375.45px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
