# Batch Size & Epoch #

출처 : 블록과 함께하는 파이썬 딥러닝 케라스

같은 문제집이라도 사람마다 푸는 방식이 다르고 학습된 결과도 다릅니다.<br>
딥러닝 모델의 학습도 비슷합니다. 케라스에서는 모델을 학습시킬 때 fit() 함수를 사용하는데, 그 인자에 따라 학습과정 및 결과가 차이가 납니다.<br>

- 케라스에서 만들 모델을 학습할 때에는 fit() 함수를 사용합니다.<br>

	`model.fit(x, y, batch_size = 32, epochs = 10)	`<br>

주요 인자는 다음과 같습니다.<br>
- x : 입력 데이터
- y : 라벨값
- batch_size : 몇 개의 샘플로 가중치를 갱신할 것인지 지정
- epochs : 학습 반복 횟수

학습에 관련된 인자이므로 시험 공부하는 것에 비유를 해보겠습니다. <br>
먼저 모의고사 1회분을 가지고 학습해 봅시다. 이 1회분은 100문항이 있고, 해답지도 제공됩니다. 문제를 푼 뒤 해답지와 맞춰보면서 학습이 이루어지기 때문에 해답지가 없으면 학습이 안됩니다.

![1](https://i.imgur.com/YRiiTkV.png)

위의 주요 인자는 다음과 같이 비유할 수 있습니다.<br>
**- x** : 100 문항의 문제들 입니다.<br>
**- y** : 100 문항의 정답들 입니다.<br>
**- batch_size** (배치 사이즈) : 배치사이즈는 몇 문항을 풀고 해답을 맞추는 지를 의미합니다.<br>

100 문항일 때, 배치사이즈가 100이면 전체를 다 풀고 난 뒤에 해답을 맞춰 보는 것입니다. 이러한 과정을 통해 모델의 가중치가 갱신 됩니다.<br>
문제를 푼뒤 해답과 맞춰봐야 학습이 일어납니다. 모델의 결과값과 주어진 라벨값과의 오차를 줄이기 위해 Back-Propagation 알고리즘으로 가중치가 갱신됩니다.

![2](https://i.imgur.com/G7rLaaT.png)

배치 사이즈가 10이면 열 문제씩 풀어보고 해답을 맞춰보는 것입니다.<br>
100 문항을 10문제씩 나우어서 10번 해답을 맞추므로 가중치 갱신은 10번 일어납니다. <br>

![3](https://i.imgur.com/kYAaxSx.png)

배치 사이즈가 1이면 한 문제 풀고 해답을 맞춰보고 또 한 문제 풀고 맞춰보고 하는 것 입니다. <br>
한 문제를 풀 때마다 가중치 갱신이 일어나므로 횟수는 100번 입니다. <bㅇr>

100문제를 다 풀고 해답을 맞추는 것과 1문제씩 풀고 해답을 맞추는 것은 어던 차이가 있을까요?<br>
모의고사 1회분에 비슷한 문항이 있다고 가정했을 때, 배치 사이즈가 100일 때는 다 풀어보고 해답을 맞춰보기 때문에 한 문제를 틀릴 경우 이후 유사 문제를 모두 틀릴 확률이 높습니다. 배치 사이즈가 1인 경우에는 한 문제씩 풀어보고 해답을 맞춰보기 때문에 유사 문제 중 첫 문제를 틀렸다고 하더라도 해답을 보면서 학습하게 되므로 나머지 문제는 맞히게 됩니다.<br>

배치 사이즈는 사람이 학습하는 것과 유사합니다.

100 문항 다 풀고 해답과 맞추어 보려면 문제가 무엇이엇는지 다 기억을 해야 맞춰보면서 학습하면 됩니다.<br>
단, 이럴 때 기억력(메모리 용량)이 좋아햐 합니다. 1문항 씩 풀고 해답 맞추면 학습은 곰꼼히 잘 되겠지만 시간이 많이 걸리게 됩니다.

<span style="background-color: #FFFF00">batch size가 작을수록 가중치 갱신이 자주 일어나서 학습이 잘 되지만 시간이 오래 걸립니다.</span>

**- epochs** <br>
에포크는 모의고사 1회분을 몇 번 풀어볼까에 해당합니다. 즉, 100 문항의 문제들을 몇 번이나 반복해서 풀어보는 지 정하는 것입니다. 에포크가 20이면 모의고사 1회분을 20번 푸는 것입니다. 처음에는 같은 문제를 반복적으로 풀어보는 것이 무슨 효과가 있는지 의문이 들겠지만 우리가 같은 문제집을 여러번 풀면서 점차 학습하듯이 모델도 같은 데이터셋으로 반복적으로 가중치를 갱신하면서 학습합니다. 같은 문제라도 이전에 풀었을 때와 지금 풀었을 때의 학습 상태가 다르기 때문에 다시 학습이 일어 납니다.<br>
즉, 같은 문제집이라도 반복해서 풀면 학습이 일어납니다.

![4](https://i.imgur.com/bR93i2j.png)

위 그래프에서 세로 축을 푼 문제 중 틀린 갯수, 가로축은 모의고사를 반복해서 푼 횟수라고 생각해 봅시다.<br>
풀이를 반복할수록 틀린 갯수가 적어지는 것을 볼 수 잇습니다. 처음에는 틀린 갯수가 급격히 줄어들지만 반복이 늘어날수록 완만해 집니다. 우리가 공부할 때도 낮은 점수에서는 공부를 조금만 해도 점수가 확 오르지만, 높은 점수에서 1 ~ 2점 올리는 것이 쉽지 않은 것과 비슷합니다.

딥러닝 학습과 실제 사람의 학습은 유사한 점들이 있습니다. 동일한 데이터를 여러번 학습하는 것과 새로운 데이터를 매번 학습하는 것 중 어떤 것이 학습에 좋을까요? 보통 공부할 때 오답노트도 만들면서 계속 반복 학습을 하면서 완전히 본인 것으로 만들듯이 동일 데이터를 반복 학습하는것이 좋습니다. 물론 어느 시점 부터는 새로운 데이터를 학습해야 합니다.

하나의 문제집만 계속 학습하면 오히려 역효과가 발생할 수 있습니다. 풀었던 문제, 연습한 문제에는 완벽하게 알 지 몰라도 새로운 문제에는 또 취약해집니다. 이런 현상을 overfitting 이라고 부릅니다. 실제로 모델을 학습할 때에도 overfitting이 일어나는지 체크하닥 조짐이 보이면 학습을 중단합니다.

